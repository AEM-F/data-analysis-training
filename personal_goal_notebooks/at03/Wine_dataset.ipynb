{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFopIDwcj4eR"
   },
   "source": [
    "# NN2 (wine classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ui7djQN6_oxN"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hT6atVUN_2qh"
   },
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "WlgWzsdq_-N9",
    "outputId": "ae276a1b-a97b-4c6e-ad45-169ceb352fc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = wine.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "ZJsmHbR9AJQx",
    "outputId": "5a4ac8af-0624-48a4-c6a1-318d669c5777"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "ywNWxtC7APmQ",
    "outputId": "2f291c10-56fd-4027-fa58-091e9b562209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = wine.target\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bw-qvoPWAcjm",
    "outputId": "30361d92-0975-49e4-d51b-337c876430bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5SyaocNLAgiZ",
    "outputId": "31fabec8-688d-45cf-da96-cb498ee2d6f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "U7mhrjwxAkmW",
    "outputId": "4ca21ea8-b280-43cc-f2d6-23a740342273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF29OkpYmMsV"
   },
   "source": [
    "## Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RGXWZc5fBaEI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4BZ631mgB71t",
    "outputId": "4d1fb0e8-769c-485e-ac88-ab1a9a6e59e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xxIE6aWOCBug",
    "outputId": "42581444-9200-47d9-b3ad-2ada4b4e7ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Lz9VXQm_CLrV",
    "outputId": "0284a14c-8071-486c-cc97-c2425f1d7037"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ic-v5HmMCVGD",
    "outputId": "23a9ce40-2225-4c32-c96f-1ab4f428b4d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S5gJUwomQ3U"
   },
   "source": [
    "## Neural network (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "duOEv22KMHLE",
    "outputId": "05d9a646-aac7-4d1c-b701-430c51d732c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13 + 3) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E22nm4baD8eO",
    "outputId": "7e6b24ae-75b2-4e57-a5ca-e93327483a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14786119\n",
      "Iteration 2, loss = 1.14602425\n",
      "Iteration 3, loss = 1.14424278\n",
      "Iteration 4, loss = 1.14251358\n",
      "Iteration 5, loss = 1.14083404\n",
      "Iteration 6, loss = 1.13920211\n",
      "Iteration 7, loss = 1.13761614\n",
      "Iteration 8, loss = 1.13607471\n",
      "Iteration 9, loss = 1.13457652\n",
      "Iteration 10, loss = 1.13312025\n",
      "Iteration 11, loss = 1.13170449\n",
      "Iteration 12, loss = 1.13032764\n",
      "Iteration 13, loss = 1.12898791\n",
      "Iteration 14, loss = 1.12768331\n",
      "Iteration 15, loss = 1.12641174\n",
      "Iteration 16, loss = 1.12517100\n",
      "Iteration 17, loss = 1.12395888\n",
      "Iteration 18, loss = 1.12277317\n",
      "Iteration 19, loss = 1.12161162\n",
      "Iteration 20, loss = 1.12047188\n",
      "Iteration 21, loss = 1.11935122\n",
      "Iteration 22, loss = 1.11824627\n",
      "Iteration 23, loss = 1.11715252\n",
      "Iteration 24, loss = 1.11606383\n",
      "Iteration 25, loss = 1.11497171\n",
      "Iteration 26, loss = 1.11386450\n",
      "Iteration 27, loss = 1.11272635\n",
      "Iteration 28, loss = 1.11153663\n",
      "Iteration 29, loss = 1.11027079\n",
      "Iteration 30, loss = 1.10890276\n",
      "Iteration 31, loss = 1.10740594\n",
      "Iteration 32, loss = 1.10574949\n",
      "Iteration 33, loss = 1.10389007\n",
      "Iteration 34, loss = 1.10177633\n",
      "Iteration 35, loss = 1.09939765\n",
      "Iteration 36, loss = 1.09683309\n",
      "Iteration 37, loss = 1.09424409\n",
      "Iteration 38, loss = 1.09198957\n",
      "Iteration 39, loss = 1.09075047\n",
      "Iteration 40, loss = 1.09097398\n",
      "Iteration 41, loss = 1.09172295\n",
      "Iteration 42, loss = 1.09191733\n",
      "Iteration 43, loss = 1.09142917\n",
      "Iteration 44, loss = 1.09043226\n",
      "Iteration 45, loss = 1.08916265\n",
      "Iteration 46, loss = 1.08791215\n",
      "Iteration 47, loss = 1.08695854\n",
      "Iteration 48, loss = 1.08641324\n",
      "Iteration 49, loss = 1.08618322\n",
      "Iteration 50, loss = 1.08608466\n",
      "Iteration 51, loss = 1.08596497\n",
      "Iteration 52, loss = 1.08574285\n",
      "Iteration 53, loss = 1.08539394\n",
      "Iteration 54, loss = 1.08492882\n",
      "Iteration 55, loss = 1.08437997\n",
      "Iteration 56, loss = 1.08379586\n",
      "Iteration 57, loss = 1.08323549\n",
      "Iteration 58, loss = 1.08275687\n",
      "Iteration 59, loss = 1.08239672\n",
      "Iteration 60, loss = 1.08214978\n",
      "Iteration 61, loss = 1.08196622\n",
      "Iteration 62, loss = 1.08177621\n",
      "Iteration 63, loss = 1.08152607\n",
      "Iteration 64, loss = 1.08120136\n",
      "Iteration 65, loss = 1.08082623\n",
      "Iteration 66, loss = 1.08044511\n",
      "Iteration 67, loss = 1.08009908\n",
      "Iteration 68, loss = 1.07980853\n",
      "Iteration 69, loss = 1.07956910\n",
      "Iteration 70, loss = 1.07935962\n",
      "Iteration 71, loss = 1.07915495\n",
      "Iteration 72, loss = 1.07893615\n",
      "Iteration 73, loss = 1.07869514\n",
      "Iteration 74, loss = 1.07843482\n",
      "Iteration 75, loss = 1.07816611\n",
      "Iteration 76, loss = 1.07790337\n",
      "Iteration 77, loss = 1.07765886\n",
      "Iteration 78, loss = 1.07743788\n",
      "Iteration 79, loss = 1.07723671\n",
      "Iteration 80, loss = 1.07704479\n",
      "Iteration 81, loss = 1.07685020\n",
      "Iteration 82, loss = 1.07664557\n",
      "Iteration 83, loss = 1.07643093\n",
      "Iteration 84, loss = 1.07621244\n",
      "Iteration 85, loss = 1.07599827\n",
      "Iteration 86, loss = 1.07579420\n",
      "Iteration 87, loss = 1.07560122\n",
      "Iteration 88, loss = 1.07541598\n",
      "Iteration 89, loss = 1.07523314\n",
      "Iteration 90, loss = 1.07504810\n",
      "Iteration 91, loss = 1.07485885\n",
      "Iteration 92, loss = 1.07466635\n",
      "Iteration 93, loss = 1.07447357\n",
      "Iteration 94, loss = 1.07428380\n",
      "Iteration 95, loss = 1.07409896\n",
      "Iteration 96, loss = 1.07391878\n",
      "Iteration 97, loss = 1.07374110\n",
      "Iteration 98, loss = 1.07356319\n",
      "Iteration 99, loss = 1.07338325\n",
      "Iteration 100, loss = 1.07320119\n",
      "Iteration 101, loss = 1.07301834\n",
      "Iteration 102, loss = 1.07283638\n",
      "Iteration 103, loss = 1.07265595\n",
      "Iteration 104, loss = 1.07247386\n",
      "Iteration 105, loss = 1.07227197\n",
      "Iteration 106, loss = 1.07199966\n",
      "Iteration 107, loss = 1.07161024\n",
      "Iteration 108, loss = 1.07108909\n",
      "Iteration 109, loss = 1.07043546\n",
      "Iteration 110, loss = 1.06977878\n",
      "Iteration 111, loss = 1.06931233\n",
      "Iteration 112, loss = 1.06920505\n",
      "Iteration 113, loss = 1.06906178\n",
      "Iteration 114, loss = 1.06867733\n",
      "Iteration 115, loss = 1.06812183\n",
      "Iteration 116, loss = 1.06752739\n",
      "Iteration 117, loss = 1.06702011\n",
      "Iteration 118, loss = 1.06662481\n",
      "Iteration 119, loss = 1.06618586\n",
      "Iteration 120, loss = 1.06547890\n",
      "Iteration 121, loss = 1.06437766\n",
      "Iteration 122, loss = 1.06326732\n",
      "Iteration 123, loss = 1.06335749\n",
      "Iteration 124, loss = 1.06286130\n",
      "Iteration 125, loss = 1.06199233\n",
      "Iteration 126, loss = 1.06125043\n",
      "Iteration 127, loss = 1.06083254\n",
      "Iteration 128, loss = 1.06039637\n",
      "Iteration 129, loss = 1.05980693\n",
      "Iteration 130, loss = 1.05911523\n",
      "Iteration 131, loss = 1.05841144\n",
      "Iteration 132, loss = 1.05780679\n",
      "Iteration 133, loss = 1.05733010\n",
      "Iteration 134, loss = 1.05680037\n",
      "Iteration 135, loss = 1.05612276\n",
      "Iteration 136, loss = 1.05541881\n",
      "Iteration 137, loss = 1.05481127\n",
      "Iteration 138, loss = 1.05426988\n",
      "Iteration 139, loss = 1.05369290\n",
      "Iteration 140, loss = 1.05304339\n",
      "Iteration 141, loss = 1.05236200\n",
      "Iteration 142, loss = 1.05170858\n",
      "Iteration 143, loss = 1.05110150\n",
      "Iteration 144, loss = 1.05049984\n",
      "Iteration 145, loss = 1.04985593\n",
      "Iteration 146, loss = 1.04917744\n",
      "Iteration 147, loss = 1.04850729\n",
      "Iteration 148, loss = 1.04786283\n",
      "Iteration 149, loss = 1.04722210\n",
      "Iteration 150, loss = 1.04656045\n",
      "Iteration 151, loss = 1.04587919\n",
      "Iteration 152, loss = 1.04519577\n",
      "Iteration 153, loss = 1.04451969\n",
      "Iteration 154, loss = 1.04384574\n",
      "Iteration 155, loss = 1.04316430\n",
      "Iteration 156, loss = 1.04247200\n",
      "Iteration 157, loss = 1.04177303\n",
      "Iteration 158, loss = 1.04107255\n",
      "Iteration 159, loss = 1.04037126\n",
      "Iteration 160, loss = 1.03966680\n",
      "Iteration 161, loss = 1.03895713\n",
      "Iteration 162, loss = 1.03824054\n",
      "Iteration 163, loss = 1.03751734\n",
      "Iteration 164, loss = 1.03679173\n",
      "Iteration 165, loss = 1.03606632\n",
      "Iteration 166, loss = 1.03533702\n",
      "Iteration 167, loss = 1.03459893\n",
      "Iteration 168, loss = 1.03385367\n",
      "Iteration 169, loss = 1.03310686\n",
      "Iteration 170, loss = 1.03235999\n",
      "Iteration 171, loss = 1.03160824\n",
      "Iteration 172, loss = 1.03084764\n",
      "Iteration 173, loss = 1.03008127\n",
      "Iteration 174, loss = 1.02931412\n",
      "Iteration 175, loss = 1.02854512\n",
      "Iteration 176, loss = 1.02776947\n",
      "Iteration 177, loss = 1.02698639\n",
      "Iteration 178, loss = 1.02619969\n",
      "Iteration 179, loss = 1.02541129\n",
      "Iteration 180, loss = 1.02461855\n",
      "Iteration 181, loss = 1.02381917\n",
      "Iteration 182, loss = 1.02301462\n",
      "Iteration 183, loss = 1.02220696\n",
      "Iteration 184, loss = 1.02139561\n",
      "Iteration 185, loss = 1.02057896\n",
      "Iteration 186, loss = 1.01975702\n",
      "Iteration 187, loss = 1.01893084\n",
      "Iteration 188, loss = 1.01810056\n",
      "Iteration 189, loss = 1.01726564\n",
      "Iteration 190, loss = 1.01642595\n",
      "Iteration 191, loss = 1.01558159\n",
      "Iteration 192, loss = 1.01473258\n",
      "Iteration 193, loss = 1.01387907\n",
      "Iteration 194, loss = 1.01302116\n",
      "Iteration 195, loss = 1.01215856\n",
      "Iteration 196, loss = 1.01129100\n",
      "Iteration 197, loss = 1.01041882\n",
      "Iteration 198, loss = 1.00954237\n",
      "Iteration 199, loss = 1.00866130\n",
      "Iteration 200, loss = 1.00777523\n",
      "Iteration 201, loss = 1.00688449\n",
      "Iteration 202, loss = 1.00598942\n",
      "Iteration 203, loss = 1.00508974\n",
      "Iteration 204, loss = 1.00418514\n",
      "Iteration 205, loss = 1.00327586\n",
      "Iteration 206, loss = 1.00236214\n",
      "Iteration 207, loss = 1.00144379\n",
      "Iteration 208, loss = 1.00052067\n",
      "Iteration 209, loss = 0.99959290\n",
      "Iteration 210, loss = 0.99866058\n",
      "Iteration 211, loss = 0.99772363\n",
      "Iteration 212, loss = 0.99678202\n",
      "Iteration 213, loss = 0.99583579\n",
      "Iteration 214, loss = 0.99488494\n",
      "Iteration 215, loss = 0.99392950\n",
      "Iteration 216, loss = 0.99296950\n",
      "Iteration 217, loss = 0.99200488\n",
      "Iteration 218, loss = 0.99103565\n",
      "Iteration 219, loss = 0.99006189\n",
      "Iteration 220, loss = 0.98908360\n",
      "Iteration 221, loss = 0.98810072\n",
      "Iteration 222, loss = 0.98711329\n",
      "Iteration 223, loss = 0.98612139\n",
      "Iteration 224, loss = 0.98512499\n",
      "Iteration 225, loss = 0.98412405\n",
      "Iteration 226, loss = 0.98311865\n",
      "Iteration 227, loss = 0.98210881\n",
      "Iteration 228, loss = 0.98109451\n",
      "Iteration 229, loss = 0.98007578\n",
      "Iteration 230, loss = 0.97905264\n",
      "Iteration 231, loss = 0.97802511\n",
      "Iteration 232, loss = 0.97699320\n",
      "Iteration 233, loss = 0.97595694\n",
      "Iteration 234, loss = 0.97491634\n",
      "Iteration 235, loss = 0.97387143\n",
      "Iteration 236, loss = 0.97282223\n",
      "Iteration 237, loss = 0.97176875\n",
      "Iteration 238, loss = 0.97071102\n",
      "Iteration 239, loss = 0.96964906\n",
      "Iteration 240, loss = 0.96858290\n",
      "Iteration 241, loss = 0.96751255\n",
      "Iteration 242, loss = 0.96643805\n",
      "Iteration 243, loss = 0.96535941\n",
      "Iteration 244, loss = 0.96427666\n",
      "Iteration 245, loss = 0.96318982\n",
      "Iteration 246, loss = 0.96209892\n",
      "Iteration 247, loss = 0.96100399\n",
      "Iteration 248, loss = 0.95990504\n",
      "Iteration 249, loss = 0.95880211\n",
      "Iteration 250, loss = 0.95769522\n",
      "Iteration 251, loss = 0.95658440\n",
      "Iteration 252, loss = 0.95546968\n",
      "Iteration 253, loss = 0.95435107\n",
      "Iteration 254, loss = 0.95322860\n",
      "Iteration 255, loss = 0.95210231\n",
      "Iteration 256, loss = 0.95097222\n",
      "Iteration 257, loss = 0.94983835\n",
      "Iteration 258, loss = 0.94870073\n",
      "Iteration 259, loss = 0.94755938\n",
      "Iteration 260, loss = 0.94641433\n",
      "Iteration 261, loss = 0.94526561\n",
      "Iteration 262, loss = 0.94411323\n",
      "Iteration 263, loss = 0.94295722\n",
      "Iteration 264, loss = 0.94179760\n",
      "Iteration 265, loss = 0.94063440\n",
      "Iteration 266, loss = 0.93946764\n",
      "Iteration 267, loss = 0.93829733\n",
      "Iteration 268, loss = 0.93712349\n",
      "Iteration 269, loss = 0.93594615\n",
      "Iteration 270, loss = 0.93476531\n",
      "Iteration 271, loss = 0.93358100\n",
      "Iteration 272, loss = 0.93239321\n",
      "Iteration 273, loss = 0.93120198\n",
      "Iteration 274, loss = 0.93000729\n",
      "Iteration 275, loss = 0.92880917\n",
      "Iteration 276, loss = 0.92760760\n",
      "Iteration 277, loss = 0.92640261\n",
      "Iteration 278, loss = 0.92519417\n",
      "Iteration 279, loss = 0.92398230\n",
      "Iteration 280, loss = 0.92276698\n",
      "Iteration 281, loss = 0.92154819\n",
      "Iteration 282, loss = 0.92032593\n",
      "Iteration 283, loss = 0.91910018\n",
      "Iteration 284, loss = 0.91787090\n",
      "Iteration 285, loss = 0.91663806\n",
      "Iteration 286, loss = 0.91540157\n",
      "Iteration 287, loss = 0.91416123\n",
      "Iteration 288, loss = 0.91291646\n",
      "Iteration 289, loss = 0.91166544\n",
      "Iteration 290, loss = 0.91040238\n",
      "Iteration 291, loss = 0.90910859\n",
      "Iteration 292, loss = 0.90772516\n",
      "Iteration 293, loss = 0.90609753\n",
      "Iteration 294, loss = 0.90420924\n",
      "Iteration 295, loss = 0.90363028\n",
      "Iteration 296, loss = 0.90203773\n",
      "Iteration 297, loss = 0.89973336\n",
      "Iteration 298, loss = 0.89809658\n",
      "Iteration 299, loss = 0.89676692\n",
      "Iteration 300, loss = 0.89493258\n",
      "Iteration 301, loss = 0.89273204\n",
      "Iteration 302, loss = 0.89114374\n",
      "Iteration 303, loss = 0.88951260\n",
      "Iteration 304, loss = 0.88726964\n",
      "Iteration 305, loss = 0.88543612\n",
      "Iteration 306, loss = 0.88374338\n",
      "Iteration 307, loss = 0.88175722\n",
      "Iteration 308, loss = 0.87966636\n",
      "Iteration 309, loss = 0.87786054\n",
      "Iteration 310, loss = 0.87604577\n",
      "Iteration 311, loss = 0.87398030\n",
      "Iteration 312, loss = 0.87203898\n",
      "Iteration 313, loss = 0.87022303\n",
      "Iteration 314, loss = 0.86831322\n",
      "Iteration 315, loss = 0.86632028\n",
      "Iteration 316, loss = 0.86442821\n",
      "Iteration 317, loss = 0.86260975\n",
      "Iteration 318, loss = 0.86069287\n",
      "Iteration 319, loss = 0.85876302\n",
      "Iteration 320, loss = 0.85692674\n",
      "Iteration 321, loss = 0.85509749\n",
      "Iteration 322, loss = 0.85321231\n",
      "Iteration 323, loss = 0.85134389\n",
      "Iteration 324, loss = 0.84953675\n",
      "Iteration 325, loss = 0.84772347\n",
      "Iteration 326, loss = 0.84587891\n",
      "Iteration 327, loss = 0.84406184\n",
      "Iteration 328, loss = 0.84228026\n",
      "Iteration 329, loss = 0.84049129\n",
      "Iteration 330, loss = 0.83869009\n",
      "Iteration 331, loss = 0.83691102\n",
      "Iteration 332, loss = 0.83515810\n",
      "Iteration 333, loss = 0.83339819\n",
      "Iteration 334, loss = 0.83163401\n",
      "Iteration 335, loss = 0.82989268\n",
      "Iteration 336, loss = 0.82816614\n",
      "Iteration 337, loss = 0.82643569\n",
      "Iteration 338, loss = 0.82470950\n",
      "Iteration 339, loss = 0.82300063\n",
      "Iteration 340, loss = 0.82130211\n",
      "Iteration 341, loss = 0.81960421\n",
      "Iteration 342, loss = 0.81791280\n",
      "Iteration 343, loss = 0.81623550\n",
      "Iteration 344, loss = 0.81456737\n",
      "Iteration 345, loss = 0.81290207\n",
      "Iteration 346, loss = 0.81124462\n",
      "Iteration 347, loss = 0.80959991\n",
      "Iteration 348, loss = 0.80796291\n",
      "Iteration 349, loss = 0.80633051\n",
      "Iteration 350, loss = 0.80470704\n",
      "Iteration 351, loss = 0.80309446\n",
      "Iteration 352, loss = 0.80148928\n",
      "Iteration 353, loss = 0.79989004\n",
      "Iteration 354, loss = 0.79829982\n",
      "Iteration 355, loss = 0.79671953\n",
      "Iteration 356, loss = 0.79514624\n",
      "Iteration 357, loss = 0.79357968\n",
      "Iteration 358, loss = 0.79202224\n",
      "Iteration 359, loss = 0.79047355\n",
      "Iteration 360, loss = 0.78893192\n",
      "Iteration 361, loss = 0.78739766\n",
      "Iteration 362, loss = 0.78587196\n",
      "Iteration 363, loss = 0.78435449\n",
      "Iteration 364, loss = 0.78284414\n",
      "Iteration 365, loss = 0.78134120\n",
      "Iteration 366, loss = 0.77984657\n",
      "Iteration 367, loss = 0.77835969\n",
      "Iteration 368, loss = 0.77687993\n",
      "Iteration 369, loss = 0.77540775\n",
      "Iteration 370, loss = 0.77394362\n",
      "Iteration 371, loss = 0.77248731\n",
      "Iteration 372, loss = 0.77103896\n",
      "Iteration 373, loss = 0.76960019\n",
      "Iteration 374, loss = 0.76817282\n",
      "Iteration 375, loss = 0.76675923\n",
      "Iteration 376, loss = 0.76535016\n",
      "Iteration 377, loss = 0.76393355\n",
      "Iteration 378, loss = 0.76249980\n",
      "Iteration 379, loss = 0.76108960\n",
      "Iteration 380, loss = 0.75971259\n",
      "Iteration 381, loss = 0.75833291\n",
      "Iteration 382, loss = 0.75693854\n",
      "Iteration 383, loss = 0.75555789\n",
      "Iteration 384, loss = 0.75420297\n",
      "Iteration 385, loss = 0.75284558\n",
      "Iteration 386, loss = 0.75147916\n",
      "Iteration 387, loss = 0.75013152\n",
      "Iteration 388, loss = 0.74879845\n",
      "Iteration 389, loss = 0.74745800\n",
      "Iteration 390, loss = 0.74612149\n",
      "Iteration 391, loss = 0.74480187\n",
      "Iteration 392, loss = 0.74348578\n",
      "Iteration 393, loss = 0.74216720\n",
      "Iteration 394, loss = 0.74085820\n",
      "Iteration 395, loss = 0.73955992\n",
      "Iteration 396, loss = 0.73826183\n",
      "Iteration 397, loss = 0.73696588\n",
      "Iteration 398, loss = 0.73568025\n",
      "Iteration 399, loss = 0.73440056\n",
      "Iteration 400, loss = 0.73312126\n",
      "Iteration 401, loss = 0.73184756\n",
      "Iteration 402, loss = 0.73058235\n",
      "Iteration 403, loss = 0.72932095\n",
      "Iteration 404, loss = 0.72806219\n",
      "Iteration 405, loss = 0.72680988\n",
      "Iteration 406, loss = 0.72556477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 407, loss = 0.72432392\n",
      "Iteration 408, loss = 0.72308712\n",
      "Iteration 409, loss = 0.72185718\n",
      "Iteration 410, loss = 0.72063441\n",
      "Iteration 411, loss = 0.71941696\n",
      "Iteration 412, loss = 0.71820497\n",
      "Iteration 413, loss = 0.71700010\n",
      "Iteration 414, loss = 0.71580296\n",
      "Iteration 415, loss = 0.71461265\n",
      "Iteration 416, loss = 0.71342881\n",
      "Iteration 417, loss = 0.71225242\n",
      "Iteration 418, loss = 0.71108426\n",
      "Iteration 419, loss = 0.70992410\n",
      "Iteration 420, loss = 0.70877151\n",
      "Iteration 421, loss = 0.70762655\n",
      "Iteration 422, loss = 0.70648977\n",
      "Iteration 423, loss = 0.70536145\n",
      "Iteration 424, loss = 0.70424143\n",
      "Iteration 425, loss = 0.70312943\n",
      "Iteration 426, loss = 0.70202530\n",
      "Iteration 427, loss = 0.70092918\n",
      "Iteration 428, loss = 0.69984115\n",
      "Iteration 429, loss = 0.69876116\n",
      "Iteration 430, loss = 0.69768905\n",
      "Iteration 431, loss = 0.69662458\n",
      "Iteration 432, loss = 0.69556757\n",
      "Iteration 433, loss = 0.69451790\n",
      "Iteration 434, loss = 0.69347548\n",
      "Iteration 435, loss = 0.69244022\n",
      "Iteration 436, loss = 0.69141201\n",
      "Iteration 437, loss = 0.69039073\n",
      "Iteration 438, loss = 0.68937626\n",
      "Iteration 439, loss = 0.68836849\n",
      "Iteration 440, loss = 0.68736730\n",
      "Iteration 441, loss = 0.68637263\n",
      "Iteration 442, loss = 0.68538438\n",
      "Iteration 443, loss = 0.68440251\n",
      "Iteration 444, loss = 0.68342696\n",
      "Iteration 445, loss = 0.68245766\n",
      "Iteration 446, loss = 0.68149436\n",
      "Iteration 447, loss = 0.68053616\n",
      "Iteration 448, loss = 0.67957948\n",
      "Iteration 449, loss = 0.67861116\n",
      "Iteration 450, loss = 0.67758615\n",
      "Iteration 451, loss = 0.67638489\n",
      "Iteration 452, loss = 0.67492837\n",
      "Iteration 453, loss = 0.67416877\n",
      "Iteration 454, loss = 0.67297301\n",
      "Iteration 455, loss = 0.67151963\n",
      "Iteration 456, loss = 0.67052113\n",
      "Iteration 457, loss = 0.66951332\n",
      "Iteration 458, loss = 0.66825812\n",
      "Iteration 459, loss = 0.66691221\n",
      "Iteration 460, loss = 0.66584996\n",
      "Iteration 461, loss = 0.66479242\n",
      "Iteration 462, loss = 0.66346308\n",
      "Iteration 463, loss = 0.66229905\n",
      "Iteration 464, loss = 0.66124790\n",
      "Iteration 465, loss = 0.66011018\n",
      "Iteration 466, loss = 0.65889430\n",
      "Iteration 467, loss = 0.65773473\n",
      "Iteration 468, loss = 0.65669051\n",
      "Iteration 469, loss = 0.65558548\n",
      "Iteration 470, loss = 0.65442845\n",
      "Iteration 471, loss = 0.65335987\n",
      "Iteration 472, loss = 0.65233371\n",
      "Iteration 473, loss = 0.65128133\n",
      "Iteration 474, loss = 0.65020823\n",
      "Iteration 475, loss = 0.64916985\n",
      "Iteration 476, loss = 0.64818561\n",
      "Iteration 477, loss = 0.64718212\n",
      "Iteration 478, loss = 0.64615948\n",
      "Iteration 479, loss = 0.64518073\n",
      "Iteration 480, loss = 0.64422663\n",
      "Iteration 481, loss = 0.64326290\n",
      "Iteration 482, loss = 0.64229681\n",
      "Iteration 483, loss = 0.64136013\n",
      "Iteration 484, loss = 0.64044825\n",
      "Iteration 485, loss = 0.63952751\n",
      "Iteration 486, loss = 0.63861344\n",
      "Iteration 487, loss = 0.63772791\n",
      "Iteration 488, loss = 0.63685266\n",
      "Iteration 489, loss = 0.63597559\n",
      "Iteration 490, loss = 0.63511009\n",
      "Iteration 491, loss = 0.63426646\n",
      "Iteration 492, loss = 0.63342838\n",
      "Iteration 493, loss = 0.63259202\n",
      "Iteration 494, loss = 0.63177181\n",
      "Iteration 495, loss = 0.63096571\n",
      "Iteration 496, loss = 0.63016313\n",
      "Iteration 497, loss = 0.62936847\n",
      "Iteration 498, loss = 0.62859051\n",
      "Iteration 499, loss = 0.62782568\n",
      "Iteration 500, loss = 0.62707467\n",
      "Iteration 501, loss = 0.62635764\n",
      "Iteration 502, loss = 0.62569275\n",
      "Iteration 503, loss = 0.62507380\n",
      "Iteration 504, loss = 0.62439802\n",
      "Iteration 505, loss = 0.62354249\n",
      "Iteration 506, loss = 0.62264658\n",
      "Iteration 507, loss = 0.62196163\n",
      "Iteration 508, loss = 0.62137504\n",
      "Iteration 509, loss = 0.62059281\n",
      "Iteration 510, loss = 0.61965544\n",
      "Iteration 511, loss = 0.61919407\n",
      "Iteration 512, loss = 0.61828841\n",
      "Iteration 513, loss = 0.61744458\n",
      "Iteration 514, loss = 0.61675820\n",
      "Iteration 515, loss = 0.61602019\n",
      "Iteration 516, loss = 0.61513853\n",
      "Iteration 517, loss = 0.61423859\n",
      "Iteration 518, loss = 0.61357645\n",
      "Iteration 519, loss = 0.61269359\n",
      "Iteration 520, loss = 0.61185145\n",
      "Iteration 521, loss = 0.61109320\n",
      "Iteration 522, loss = 0.61028724\n",
      "Iteration 523, loss = 0.60945459\n",
      "Iteration 524, loss = 0.60867159\n",
      "Iteration 525, loss = 0.60793473\n",
      "Iteration 526, loss = 0.60715153\n",
      "Iteration 527, loss = 0.60637195\n",
      "Iteration 528, loss = 0.60563113\n",
      "Iteration 529, loss = 0.60490416\n",
      "Iteration 530, loss = 0.60417363\n",
      "Iteration 531, loss = 0.60344678\n",
      "Iteration 532, loss = 0.60273835\n",
      "Iteration 533, loss = 0.60204377\n",
      "Iteration 534, loss = 0.60135465\n",
      "Iteration 535, loss = 0.60066950\n",
      "Iteration 536, loss = 0.59998601\n",
      "Iteration 537, loss = 0.59931550\n",
      "Iteration 538, loss = 0.59866249\n",
      "Iteration 539, loss = 0.59800348\n",
      "Iteration 540, loss = 0.59735077\n",
      "Iteration 541, loss = 0.59672015\n",
      "Iteration 542, loss = 0.59609203\n",
      "Iteration 543, loss = 0.59545698\n",
      "Iteration 544, loss = 0.59483340\n",
      "Iteration 545, loss = 0.59422947\n",
      "Iteration 546, loss = 0.59362383\n",
      "Iteration 547, loss = 0.59301977\n",
      "Iteration 548, loss = 0.59242851\n",
      "Iteration 549, loss = 0.59183918\n",
      "Iteration 550, loss = 0.59125339\n",
      "Iteration 551, loss = 0.59068035\n",
      "Iteration 552, loss = 0.59011036\n",
      "Iteration 553, loss = 0.58954328\n",
      "Iteration 554, loss = 0.58898574\n",
      "Iteration 555, loss = 0.58842925\n",
      "Iteration 556, loss = 0.58787663\n",
      "Iteration 557, loss = 0.58733380\n",
      "Iteration 558, loss = 0.58679207\n",
      "Iteration 559, loss = 0.58625548\n",
      "Iteration 560, loss = 0.58572742\n",
      "Iteration 561, loss = 0.58520038\n",
      "Iteration 562, loss = 0.58467918\n",
      "Iteration 563, loss = 0.58416445\n",
      "Iteration 564, loss = 0.58365167\n",
      "Iteration 565, loss = 0.58314555\n",
      "Iteration 566, loss = 0.58264471\n",
      "Iteration 567, loss = 0.58214931\n",
      "Iteration 568, loss = 0.58166382\n",
      "Iteration 569, loss = 0.58118924\n",
      "Iteration 570, loss = 0.58073689\n",
      "Iteration 571, loss = 0.58031417\n",
      "Iteration 572, loss = 0.57994462\n",
      "Iteration 573, loss = 0.57957987\n",
      "Iteration 574, loss = 0.57917812\n",
      "Iteration 575, loss = 0.57855113\n",
      "Iteration 576, loss = 0.57789471\n",
      "Iteration 577, loss = 0.57740477\n",
      "Iteration 578, loss = 0.57708157\n",
      "Iteration 579, loss = 0.57671835\n",
      "Iteration 580, loss = 0.57616452\n",
      "Iteration 581, loss = 0.57561608\n",
      "Iteration 582, loss = 0.57521854\n",
      "Iteration 583, loss = 0.57486810\n",
      "Iteration 584, loss = 0.57441947\n",
      "Iteration 585, loss = 0.57390273\n",
      "Iteration 586, loss = 0.57347598\n",
      "Iteration 587, loss = 0.57311916\n",
      "Iteration 588, loss = 0.57270259\n",
      "Iteration 589, loss = 0.57223110\n",
      "Iteration 590, loss = 0.57179978\n",
      "Iteration 591, loss = 0.57142890\n",
      "Iteration 592, loss = 0.57104002\n",
      "Iteration 593, loss = 0.57060011\n",
      "Iteration 594, loss = 0.57017392\n",
      "Iteration 595, loss = 0.56979336\n",
      "Iteration 596, loss = 0.56941530\n",
      "Iteration 597, loss = 0.56900609\n",
      "Iteration 598, loss = 0.56858978\n",
      "Iteration 599, loss = 0.56820159\n",
      "Iteration 600, loss = 0.56782972\n",
      "Iteration 601, loss = 0.56744251\n",
      "Iteration 602, loss = 0.56704128\n",
      "Iteration 603, loss = 0.56664965\n",
      "Iteration 604, loss = 0.56627573\n",
      "Iteration 605, loss = 0.56590398\n",
      "Iteration 606, loss = 0.56552061\n",
      "Iteration 607, loss = 0.56513332\n",
      "Iteration 608, loss = 0.56475486\n",
      "Iteration 609, loss = 0.56438631\n",
      "Iteration 610, loss = 0.56401796\n",
      "Iteration 611, loss = 0.56364352\n",
      "Iteration 612, loss = 0.56326697\n",
      "Iteration 613, loss = 0.56289520\n",
      "Iteration 614, loss = 0.56252989\n",
      "Iteration 615, loss = 0.56216671\n",
      "Iteration 616, loss = 0.56180146\n",
      "Iteration 617, loss = 0.56143400\n",
      "Iteration 618, loss = 0.56106736\n",
      "Iteration 619, loss = 0.56070406\n",
      "Iteration 620, loss = 0.56034426\n",
      "Iteration 621, loss = 0.55998611\n",
      "Iteration 622, loss = 0.55962805\n",
      "Iteration 623, loss = 0.55926949\n",
      "Iteration 624, loss = 0.55891104\n",
      "Iteration 625, loss = 0.55855360\n",
      "Iteration 626, loss = 0.55819792\n",
      "Iteration 627, loss = 0.55784416\n",
      "Iteration 628, loss = 0.55749224\n",
      "Iteration 629, loss = 0.55714195\n",
      "Iteration 630, loss = 0.55679324\n",
      "Iteration 631, loss = 0.55644618\n",
      "Iteration 632, loss = 0.55610122\n",
      "Iteration 633, loss = 0.55575910\n",
      "Iteration 634, loss = 0.55542161\n",
      "Iteration 635, loss = 0.55509199\n",
      "Iteration 636, loss = 0.55477787\n",
      "Iteration 637, loss = 0.55448982\n",
      "Iteration 638, loss = 0.55424642\n",
      "Iteration 639, loss = 0.55402914\n",
      "Iteration 640, loss = 0.55376287\n",
      "Iteration 641, loss = 0.55334657\n",
      "Iteration 642, loss = 0.55281154\n",
      "Iteration 643, loss = 0.55239280\n",
      "Iteration 644, loss = 0.55215615\n",
      "Iteration 645, loss = 0.55194806\n",
      "Iteration 646, loss = 0.55160255\n",
      "Iteration 647, loss = 0.55114571\n",
      "Iteration 648, loss = 0.55078216\n",
      "Iteration 649, loss = 0.55054121\n",
      "Iteration 650, loss = 0.55028021\n",
      "Iteration 651, loss = 0.54990883\n",
      "Iteration 652, loss = 0.54951310\n",
      "Iteration 653, loss = 0.54921565\n",
      "Iteration 654, loss = 0.54896219\n",
      "Iteration 655, loss = 0.54865150\n",
      "Iteration 656, loss = 0.54828138\n",
      "Iteration 657, loss = 0.54794325\n",
      "Iteration 658, loss = 0.54767172\n",
      "Iteration 659, loss = 0.54738976\n",
      "Iteration 660, loss = 0.54705886\n",
      "Iteration 661, loss = 0.54671293\n",
      "Iteration 662, loss = 0.54640832\n",
      "Iteration 663, loss = 0.54613379\n",
      "Iteration 664, loss = 0.54583354\n",
      "Iteration 665, loss = 0.54550611\n",
      "Iteration 666, loss = 0.54518191\n",
      "Iteration 667, loss = 0.54488871\n",
      "Iteration 668, loss = 0.54460614\n",
      "Iteration 669, loss = 0.54430277\n",
      "Iteration 670, loss = 0.54398538\n",
      "Iteration 671, loss = 0.54367271\n",
      "Iteration 672, loss = 0.54338069\n",
      "Iteration 673, loss = 0.54309394\n",
      "Iteration 674, loss = 0.54279516\n",
      "Iteration 675, loss = 0.54248684\n",
      "Iteration 676, loss = 0.54217922\n",
      "Iteration 677, loss = 0.54188477\n",
      "Iteration 678, loss = 0.54159597\n",
      "Iteration 679, loss = 0.54130324\n",
      "Iteration 680, loss = 0.54100322\n",
      "Iteration 681, loss = 0.54069948\n",
      "Iteration 682, loss = 0.54040155\n",
      "Iteration 683, loss = 0.54010904\n",
      "Iteration 684, loss = 0.53981905\n",
      "Iteration 685, loss = 0.53952636\n",
      "Iteration 686, loss = 0.53922929\n",
      "Iteration 687, loss = 0.53893125\n",
      "Iteration 688, loss = 0.53863424\n",
      "Iteration 689, loss = 0.53834053\n",
      "Iteration 690, loss = 0.53804855\n",
      "Iteration 691, loss = 0.53775682\n",
      "Iteration 692, loss = 0.53746398\n",
      "Iteration 693, loss = 0.53716931\n",
      "Iteration 694, loss = 0.53687378\n",
      "Iteration 695, loss = 0.53657766\n",
      "Iteration 696, loss = 0.53628214\n",
      "Iteration 697, loss = 0.53598716\n",
      "Iteration 698, loss = 0.53569270\n",
      "Iteration 699, loss = 0.53539843\n",
      "Iteration 700, loss = 0.53510383\n",
      "Iteration 701, loss = 0.53480889\n",
      "Iteration 702, loss = 0.53451324\n",
      "Iteration 703, loss = 0.53421709\n",
      "Iteration 704, loss = 0.53392018\n",
      "Iteration 705, loss = 0.53362274\n",
      "Iteration 706, loss = 0.53332482\n",
      "Iteration 707, loss = 0.53302659\n",
      "Iteration 708, loss = 0.53272817\n",
      "Iteration 709, loss = 0.53242986\n",
      "Iteration 710, loss = 0.53213223\n",
      "Iteration 711, loss = 0.53183625\n",
      "Iteration 712, loss = 0.53154286\n",
      "Iteration 713, loss = 0.53125366\n",
      "Iteration 714, loss = 0.53096866\n",
      "Iteration 715, loss = 0.53068722\n",
      "Iteration 716, loss = 0.53040128\n",
      "Iteration 717, loss = 0.53009831\n",
      "Iteration 718, loss = 0.52976635\n",
      "Iteration 719, loss = 0.52940845\n",
      "Iteration 720, loss = 0.52904773\n",
      "Iteration 721, loss = 0.52870737\n",
      "Iteration 722, loss = 0.52839413\n",
      "Iteration 723, loss = 0.52809594\n",
      "Iteration 724, loss = 0.52779310\n",
      "Iteration 725, loss = 0.52747072\n",
      "Iteration 726, loss = 0.52712477\n",
      "Iteration 727, loss = 0.52676655\n",
      "Iteration 728, loss = 0.52641141\n",
      "Iteration 729, loss = 0.52606826\n",
      "Iteration 730, loss = 0.52573400\n",
      "Iteration 731, loss = 0.52539852\n",
      "Iteration 732, loss = 0.52505242\n",
      "Iteration 733, loss = 0.52469121\n",
      "Iteration 734, loss = 0.52431802\n",
      "Iteration 735, loss = 0.52393898\n",
      "Iteration 736, loss = 0.52355989\n",
      "Iteration 737, loss = 0.52318224\n",
      "Iteration 738, loss = 0.52280370\n",
      "Iteration 739, loss = 0.52242040\n",
      "Iteration 740, loss = 0.52202898\n",
      "Iteration 741, loss = 0.52162794\n",
      "Iteration 742, loss = 0.52121724\n",
      "Iteration 743, loss = 0.52079850\n",
      "Iteration 744, loss = 0.52037338\n",
      "Iteration 745, loss = 0.51994335\n",
      "Iteration 746, loss = 0.51950912\n",
      "Iteration 747, loss = 0.51907069\n",
      "Iteration 748, loss = 0.51862779\n",
      "Iteration 749, loss = 0.51818011\n",
      "Iteration 750, loss = 0.51772746\n",
      "Iteration 751, loss = 0.51726993\n",
      "Iteration 752, loss = 0.51680774\n",
      "Iteration 753, loss = 0.51634126\n",
      "Iteration 754, loss = 0.51587109\n",
      "Iteration 755, loss = 0.51539746\n",
      "Iteration 756, loss = 0.51492135\n",
      "Iteration 757, loss = 0.51444200\n",
      "Iteration 758, loss = 0.51396038\n",
      "Iteration 759, loss = 0.51347288\n",
      "Iteration 760, loss = 0.51297992\n",
      "Iteration 761, loss = 0.51247499\n",
      "Iteration 762, loss = 0.51196050\n",
      "Iteration 763, loss = 0.51143453\n",
      "Iteration 764, loss = 0.51090455\n",
      "Iteration 765, loss = 0.51037499\n",
      "Iteration 766, loss = 0.50985014\n",
      "Iteration 767, loss = 0.50933024\n",
      "Iteration 768, loss = 0.50881300\n",
      "Iteration 769, loss = 0.50829521\n",
      "Iteration 770, loss = 0.50777350\n",
      "Iteration 771, loss = 0.50724638\n",
      "Iteration 772, loss = 0.50671171\n",
      "Iteration 773, loss = 0.50617094\n",
      "Iteration 774, loss = 0.50562432\n",
      "Iteration 775, loss = 0.50507472\n",
      "Iteration 776, loss = 0.50452360\n",
      "Iteration 777, loss = 0.50397228\n",
      "Iteration 778, loss = 0.50342091\n",
      "Iteration 779, loss = 0.50286896\n",
      "Iteration 780, loss = 0.50231557\n",
      "Iteration 781, loss = 0.50175988\n",
      "Iteration 782, loss = 0.50120133\n",
      "Iteration 783, loss = 0.50063930\n",
      "Iteration 784, loss = 0.50007386\n",
      "Iteration 785, loss = 0.49950449\n",
      "Iteration 786, loss = 0.49893172\n",
      "Iteration 787, loss = 0.49835509\n",
      "Iteration 788, loss = 0.49777529\n",
      "Iteration 789, loss = 0.49719199\n",
      "Iteration 790, loss = 0.49660580\n",
      "Iteration 791, loss = 0.49601651\n",
      "Iteration 792, loss = 0.49542451\n",
      "Iteration 793, loss = 0.49482967\n",
      "Iteration 794, loss = 0.49423216\n",
      "Iteration 795, loss = 0.49363189\n",
      "Iteration 796, loss = 0.49302889\n",
      "Iteration 797, loss = 0.49242309\n",
      "Iteration 798, loss = 0.49181450\n",
      "Iteration 799, loss = 0.49120307\n",
      "Iteration 800, loss = 0.49058882\n",
      "Iteration 801, loss = 0.48997176\n",
      "Iteration 802, loss = 0.48935196\n",
      "Iteration 803, loss = 0.48872949\n",
      "Iteration 804, loss = 0.48810456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 805, loss = 0.48747738\n",
      "Iteration 806, loss = 0.48684857\n",
      "Iteration 807, loss = 0.48621864\n",
      "Iteration 808, loss = 0.48558931\n",
      "Iteration 809, loss = 0.48496136\n",
      "Iteration 810, loss = 0.48433905\n",
      "Iteration 811, loss = 0.48372038\n",
      "Iteration 812, loss = 0.48311173\n",
      "Iteration 813, loss = 0.48249214\n",
      "Iteration 814, loss = 0.48186291\n",
      "Iteration 815, loss = 0.48117899\n",
      "Iteration 816, loss = 0.48046417\n",
      "Iteration 817, loss = 0.47973218\n",
      "Iteration 818, loss = 0.47903148\n",
      "Iteration 819, loss = 0.47837410\n",
      "Iteration 820, loss = 0.47773924\n",
      "Iteration 821, loss = 0.47709509\n",
      "Iteration 822, loss = 0.47641271\n",
      "Iteration 823, loss = 0.47570291\n",
      "Iteration 824, loss = 0.47498657\n",
      "Iteration 825, loss = 0.47429014\n",
      "Iteration 826, loss = 0.47361537\n",
      "Iteration 827, loss = 0.47294446\n",
      "Iteration 828, loss = 0.47225953\n",
      "Iteration 829, loss = 0.47155191\n",
      "Iteration 830, loss = 0.47083377\n",
      "Iteration 831, loss = 0.47011894\n",
      "Iteration 832, loss = 0.46941519\n",
      "Iteration 833, loss = 0.46871784\n",
      "Iteration 834, loss = 0.46801612\n",
      "Iteration 835, loss = 0.46730358\n",
      "Iteration 836, loss = 0.46657975\n",
      "Iteration 837, loss = 0.46585190\n",
      "Iteration 838, loss = 0.46512607\n",
      "Iteration 839, loss = 0.46440412\n",
      "Iteration 840, loss = 0.46368309\n",
      "Iteration 841, loss = 0.46295822\n",
      "Iteration 842, loss = 0.46222695\n",
      "Iteration 843, loss = 0.46148917\n",
      "Iteration 844, loss = 0.46074777\n",
      "Iteration 845, loss = 0.46000535\n",
      "Iteration 846, loss = 0.45926323\n",
      "Iteration 847, loss = 0.45852084\n",
      "Iteration 848, loss = 0.45777657\n",
      "Iteration 849, loss = 0.45702896\n",
      "Iteration 850, loss = 0.45627720\n",
      "Iteration 851, loss = 0.45552171\n",
      "Iteration 852, loss = 0.45476323\n",
      "Iteration 853, loss = 0.45400275\n",
      "Iteration 854, loss = 0.45324081\n",
      "Iteration 855, loss = 0.45247751\n",
      "Iteration 856, loss = 0.45171258\n",
      "Iteration 857, loss = 0.45094558\n",
      "Iteration 858, loss = 0.45017619\n",
      "Iteration 859, loss = 0.44940416\n",
      "Iteration 860, loss = 0.44862954\n",
      "Iteration 861, loss = 0.44785237\n",
      "Iteration 862, loss = 0.44707290\n",
      "Iteration 863, loss = 0.44629127\n",
      "Iteration 864, loss = 0.44550764\n",
      "Iteration 865, loss = 0.44472209\n",
      "Iteration 866, loss = 0.44393469\n",
      "Iteration 867, loss = 0.44314544\n",
      "Iteration 868, loss = 0.44235435\n",
      "Iteration 869, loss = 0.44156142\n",
      "Iteration 870, loss = 0.44076665\n",
      "Iteration 871, loss = 0.43997006\n",
      "Iteration 872, loss = 0.43917167\n",
      "Iteration 873, loss = 0.43837152\n",
      "Iteration 874, loss = 0.43756968\n",
      "Iteration 875, loss = 0.43676621\n",
      "Iteration 876, loss = 0.43596121\n",
      "Iteration 877, loss = 0.43515485\n",
      "Iteration 878, loss = 0.43434731\n",
      "Iteration 879, loss = 0.43353903\n",
      "Iteration 880, loss = 0.43273037\n",
      "Iteration 881, loss = 0.43192253\n",
      "Iteration 882, loss = 0.43111634\n",
      "Iteration 883, loss = 0.43031507\n",
      "Iteration 884, loss = 0.42951962\n",
      "Iteration 885, loss = 0.42873794\n",
      "Iteration 886, loss = 0.42796378\n",
      "Iteration 887, loss = 0.42720790\n",
      "Iteration 888, loss = 0.42642547\n",
      "Iteration 889, loss = 0.42562003\n",
      "Iteration 890, loss = 0.42472401\n",
      "Iteration 891, loss = 0.42379766\n",
      "Iteration 892, loss = 0.42288692\n",
      "Iteration 893, loss = 0.42204886\n",
      "Iteration 894, loss = 0.42127048\n",
      "Iteration 895, loss = 0.42049338\n",
      "Iteration 896, loss = 0.41967528\n",
      "Iteration 897, loss = 0.41879712\n",
      "Iteration 898, loss = 0.41791023\n",
      "Iteration 899, loss = 0.41705675\n",
      "Iteration 900, loss = 0.41624475\n",
      "Iteration 901, loss = 0.41544333\n",
      "Iteration 902, loss = 0.41461444\n",
      "Iteration 903, loss = 0.41375599\n",
      "Iteration 904, loss = 0.41288719\n",
      "Iteration 905, loss = 0.41203710\n",
      "Iteration 906, loss = 0.41121000\n",
      "Iteration 907, loss = 0.41038805\n",
      "Iteration 908, loss = 0.40955307\n",
      "Iteration 909, loss = 0.40869920\n",
      "Iteration 910, loss = 0.40784044\n",
      "Iteration 911, loss = 0.40699036\n",
      "Iteration 912, loss = 0.40615255\n",
      "Iteration 913, loss = 0.40531894\n",
      "Iteration 914, loss = 0.40447876\n",
      "Iteration 915, loss = 0.40362935\n",
      "Iteration 916, loss = 0.40277473\n",
      "Iteration 917, loss = 0.40192286\n",
      "Iteration 918, loss = 0.40107716\n",
      "Iteration 919, loss = 0.40023536\n",
      "Iteration 920, loss = 0.39939257\n",
      "Iteration 921, loss = 0.39854511\n",
      "Iteration 922, loss = 0.39769371\n",
      "Iteration 923, loss = 0.39684110\n",
      "Iteration 924, loss = 0.39599040\n",
      "Iteration 925, loss = 0.39514247\n",
      "Iteration 926, loss = 0.39429606\n",
      "Iteration 927, loss = 0.39344912\n",
      "Iteration 928, loss = 0.39260018\n",
      "Iteration 929, loss = 0.39174938\n",
      "Iteration 930, loss = 0.39089765\n",
      "Iteration 931, loss = 0.39004633\n",
      "Iteration 932, loss = 0.38919612\n",
      "Iteration 933, loss = 0.38834695\n",
      "Iteration 934, loss = 0.38749825\n",
      "Iteration 935, loss = 0.38664930\n",
      "Iteration 936, loss = 0.38579973\n",
      "Iteration 937, loss = 0.38494948\n",
      "Iteration 938, loss = 0.38409888\n",
      "Iteration 939, loss = 0.38324829\n",
      "Iteration 940, loss = 0.38239806\n",
      "Iteration 941, loss = 0.38154832\n",
      "Iteration 942, loss = 0.38069906\n",
      "Iteration 943, loss = 0.37985018\n",
      "Iteration 944, loss = 0.37900151\n",
      "Iteration 945, loss = 0.37815295\n",
      "Iteration 946, loss = 0.37730440\n",
      "Iteration 947, loss = 0.37645574\n",
      "Iteration 948, loss = 0.37560658\n",
      "Iteration 949, loss = 0.37475584\n",
      "Iteration 950, loss = 0.37390762\n",
      "Iteration 951, loss = 0.37305572\n",
      "Iteration 952, loss = 0.37220748\n",
      "Iteration 953, loss = 0.37135681\n",
      "Iteration 954, loss = 0.37050063\n",
      "Iteration 955, loss = 0.36965419\n",
      "Iteration 956, loss = 0.36879228\n",
      "Iteration 957, loss = 0.36794430\n",
      "Iteration 958, loss = 0.36708718\n",
      "Iteration 959, loss = 0.36622030\n",
      "Iteration 960, loss = 0.36538073\n",
      "Iteration 961, loss = 0.36450281\n",
      "Iteration 962, loss = 0.36365707\n",
      "Iteration 963, loss = 0.36279322\n",
      "Iteration 964, loss = 0.36191354\n",
      "Iteration 965, loss = 0.36107301\n",
      "Iteration 966, loss = 0.36018114\n",
      "Iteration 967, loss = 0.35932728\n",
      "Iteration 968, loss = 0.35845705\n",
      "Iteration 969, loss = 0.35757109\n",
      "Iteration 970, loss = 0.35671714\n",
      "Iteration 971, loss = 0.35582249\n",
      "Iteration 972, loss = 0.35495791\n",
      "Iteration 973, loss = 0.35407620\n",
      "Iteration 974, loss = 0.35319294\n",
      "Iteration 975, loss = 0.35232676\n",
      "Iteration 976, loss = 0.35144978\n",
      "Iteration 977, loss = 0.35060595\n",
      "Iteration 978, loss = 0.34977790\n",
      "Iteration 979, loss = 0.34898618\n",
      "Iteration 980, loss = 0.34823536\n",
      "Iteration 981, loss = 0.34736597\n",
      "Iteration 982, loss = 0.34637593\n",
      "Iteration 983, loss = 0.34513613\n",
      "Iteration 984, loss = 0.34394729\n",
      "Iteration 985, loss = 0.34287045\n",
      "Iteration 986, loss = 0.34193777\n",
      "Iteration 987, loss = 0.34121747\n",
      "Iteration 988, loss = 0.34010929\n",
      "Iteration 989, loss = 0.33830951\n",
      "Iteration 990, loss = 0.33737071\n",
      "Iteration 991, loss = 0.33569362\n",
      "Iteration 992, loss = 0.33470008\n",
      "Iteration 993, loss = 0.33323838\n",
      "Iteration 994, loss = 0.33200068\n",
      "Iteration 995, loss = 0.33049836\n",
      "Iteration 996, loss = 0.32940152\n",
      "Iteration 997, loss = 0.32775421\n",
      "Iteration 998, loss = 0.32675511\n",
      "Iteration 999, loss = 0.32523074\n",
      "Iteration 1000, loss = 0.32414396\n",
      "Iteration 1001, loss = 0.32287700\n",
      "Iteration 1002, loss = 0.32169543\n",
      "Iteration 1003, loss = 0.32064992\n",
      "Iteration 1004, loss = 0.31939140\n",
      "Iteration 1005, loss = 0.31845438\n",
      "Iteration 1006, loss = 0.31720933\n",
      "Iteration 1007, loss = 0.31627199\n",
      "Iteration 1008, loss = 0.31507953\n",
      "Iteration 1009, loss = 0.31407733\n",
      "Iteration 1010, loss = 0.31297158\n",
      "Iteration 1011, loss = 0.31188852\n",
      "Iteration 1012, loss = 0.31083446\n",
      "Iteration 1013, loss = 0.30970848\n",
      "Iteration 1014, loss = 0.30867163\n",
      "Iteration 1015, loss = 0.30753913\n",
      "Iteration 1016, loss = 0.30647325\n",
      "Iteration 1017, loss = 0.30537187\n",
      "Iteration 1018, loss = 0.30426970\n",
      "Iteration 1019, loss = 0.30318947\n",
      "Iteration 1020, loss = 0.30207110\n",
      "Iteration 1021, loss = 0.30099559\n",
      "Iteration 1022, loss = 0.29988646\n",
      "Iteration 1023, loss = 0.29879676\n",
      "Iteration 1024, loss = 0.29770892\n",
      "Iteration 1025, loss = 0.29661090\n",
      "Iteration 1026, loss = 0.29553972\n",
      "Iteration 1027, loss = 0.29444056\n",
      "Iteration 1028, loss = 0.29337906\n",
      "Iteration 1029, loss = 0.29229080\n",
      "Iteration 1030, loss = 0.29123218\n",
      "Iteration 1031, loss = 0.29015866\n",
      "Iteration 1032, loss = 0.28910038\n",
      "Iteration 1033, loss = 0.28804375\n",
      "Iteration 1034, loss = 0.28698649\n",
      "Iteration 1035, loss = 0.28594159\n",
      "Iteration 1036, loss = 0.28488984\n",
      "Iteration 1037, loss = 0.28385261\n",
      "Iteration 1038, loss = 0.28280937\n",
      "Iteration 1039, loss = 0.28177578\n",
      "Iteration 1040, loss = 0.28074196\n",
      "Iteration 1041, loss = 0.27971248\n",
      "Iteration 1042, loss = 0.27868742\n",
      "Iteration 1043, loss = 0.27766241\n",
      "Iteration 1044, loss = 0.27664470\n",
      "Iteration 1045, loss = 0.27562621\n",
      "Iteration 1046, loss = 0.27461511\n",
      "Iteration 1047, loss = 0.27360421\n",
      "Iteration 1048, loss = 0.27259900\n",
      "Iteration 1049, loss = 0.27159670\n",
      "Iteration 1050, loss = 0.27059793\n",
      "Iteration 1051, loss = 0.26960418\n",
      "Iteration 1052, loss = 0.26861242\n",
      "Iteration 1053, loss = 0.26762690\n",
      "Iteration 1054, loss = 0.26664321\n",
      "Iteration 1055, loss = 0.26566554\n",
      "Iteration 1056, loss = 0.26469035\n",
      "Iteration 1057, loss = 0.26372027\n",
      "Iteration 1058, loss = 0.26275387\n",
      "Iteration 1059, loss = 0.26179157\n",
      "Iteration 1060, loss = 0.26083376\n",
      "Iteration 1061, loss = 0.25987937\n",
      "Iteration 1062, loss = 0.25892991\n",
      "Iteration 1063, loss = 0.25798377\n",
      "Iteration 1064, loss = 0.25704244\n",
      "Iteration 1065, loss = 0.25610462\n",
      "Iteration 1066, loss = 0.25517129\n",
      "Iteration 1067, loss = 0.25424187\n",
      "Iteration 1068, loss = 0.25331656\n",
      "Iteration 1069, loss = 0.25239545\n",
      "Iteration 1070, loss = 0.25147821\n",
      "Iteration 1071, loss = 0.25056532\n",
      "Iteration 1072, loss = 0.24965628\n",
      "Iteration 1073, loss = 0.24875152\n",
      "Iteration 1074, loss = 0.24785071\n",
      "Iteration 1075, loss = 0.24695403\n",
      "Iteration 1076, loss = 0.24606145\n",
      "Iteration 1077, loss = 0.24517287\n",
      "Iteration 1078, loss = 0.24428847\n",
      "Iteration 1079, loss = 0.24340800\n",
      "Iteration 1080, loss = 0.24253171\n",
      "Iteration 1081, loss = 0.24165936\n",
      "Iteration 1082, loss = 0.24079113\n",
      "Iteration 1083, loss = 0.23992689\n",
      "Iteration 1084, loss = 0.23906666\n",
      "Iteration 1085, loss = 0.23821047\n",
      "Iteration 1086, loss = 0.23735823\n",
      "Iteration 1087, loss = 0.23651003\n",
      "Iteration 1088, loss = 0.23566573\n",
      "Iteration 1089, loss = 0.23482545\n",
      "Iteration 1090, loss = 0.23398907\n",
      "Iteration 1091, loss = 0.23315665\n",
      "Iteration 1092, loss = 0.23232812\n",
      "Iteration 1093, loss = 0.23150350\n",
      "Iteration 1094, loss = 0.23068278\n",
      "Iteration 1095, loss = 0.22986591\n",
      "Iteration 1096, loss = 0.22905292\n",
      "Iteration 1097, loss = 0.22824375\n",
      "Iteration 1098, loss = 0.22743843\n",
      "Iteration 1099, loss = 0.22663691\n",
      "Iteration 1100, loss = 0.22583920\n",
      "Iteration 1101, loss = 0.22504527\n",
      "Iteration 1102, loss = 0.22425510\n",
      "Iteration 1103, loss = 0.22346869\n",
      "Iteration 1104, loss = 0.22268602\n",
      "Iteration 1105, loss = 0.22190707\n",
      "Iteration 1106, loss = 0.22113183\n",
      "Iteration 1107, loss = 0.22036027\n",
      "Iteration 1108, loss = 0.21959240\n",
      "Iteration 1109, loss = 0.21882818\n",
      "Iteration 1110, loss = 0.21806760\n",
      "Iteration 1111, loss = 0.21731065\n",
      "Iteration 1112, loss = 0.21655731\n",
      "Iteration 1113, loss = 0.21580756\n",
      "Iteration 1114, loss = 0.21506139\n",
      "Iteration 1115, loss = 0.21431878\n",
      "Iteration 1116, loss = 0.21357972\n",
      "Iteration 1117, loss = 0.21284418\n",
      "Iteration 1118, loss = 0.21211215\n",
      "Iteration 1119, loss = 0.21138362\n",
      "Iteration 1120, loss = 0.21065856\n",
      "Iteration 1121, loss = 0.20993696\n",
      "Iteration 1122, loss = 0.20921880\n",
      "Iteration 1123, loss = 0.20850408\n",
      "Iteration 1124, loss = 0.20779276\n",
      "Iteration 1125, loss = 0.20708483\n",
      "Iteration 1126, loss = 0.20638027\n",
      "Iteration 1127, loss = 0.20567908\n",
      "Iteration 1128, loss = 0.20498122\n",
      "Iteration 1129, loss = 0.20428669\n",
      "Iteration 1130, loss = 0.20359546\n",
      "Iteration 1131, loss = 0.20290753\n",
      "Iteration 1132, loss = 0.20222286\n",
      "Iteration 1133, loss = 0.20154144\n",
      "Iteration 1134, loss = 0.20086327\n",
      "Iteration 1135, loss = 0.20018830\n",
      "Iteration 1136, loss = 0.19951654\n",
      "Iteration 1137, loss = 0.19884796\n",
      "Iteration 1138, loss = 0.19818255\n",
      "Iteration 1139, loss = 0.19752028\n",
      "Iteration 1140, loss = 0.19686113\n",
      "Iteration 1141, loss = 0.19620510\n",
      "Iteration 1142, loss = 0.19555215\n",
      "Iteration 1143, loss = 0.19490227\n",
      "Iteration 1144, loss = 0.19425545\n",
      "Iteration 1145, loss = 0.19361166\n",
      "Iteration 1146, loss = 0.19297088\n",
      "Iteration 1147, loss = 0.19233309\n",
      "Iteration 1148, loss = 0.19169828\n",
      "Iteration 1149, loss = 0.19106642\n",
      "Iteration 1150, loss = 0.19043749\n",
      "Iteration 1151, loss = 0.18981147\n",
      "Iteration 1152, loss = 0.18918835\n",
      "Iteration 1153, loss = 0.18856809\n",
      "Iteration 1154, loss = 0.18795068\n",
      "Iteration 1155, loss = 0.18733610\n",
      "Iteration 1156, loss = 0.18672433\n",
      "Iteration 1157, loss = 0.18611533\n",
      "Iteration 1158, loss = 0.18550909\n",
      "Iteration 1159, loss = 0.18490559\n",
      "Iteration 1160, loss = 0.18430481\n",
      "Iteration 1161, loss = 0.18370671\n",
      "Iteration 1162, loss = 0.18311127\n",
      "Iteration 1163, loss = 0.18251847\n",
      "Iteration 1164, loss = 0.18192829\n",
      "Iteration 1165, loss = 0.18134069\n",
      "Iteration 1166, loss = 0.18075565\n",
      "Iteration 1167, loss = 0.18017315\n",
      "Iteration 1168, loss = 0.17959316\n",
      "Iteration 1169, loss = 0.17901565\n",
      "Iteration 1170, loss = 0.17844059\n",
      "Iteration 1171, loss = 0.17786796\n",
      "Iteration 1172, loss = 0.17729772\n",
      "Iteration 1173, loss = 0.17672985\n",
      "Iteration 1174, loss = 0.17616431\n",
      "Iteration 1175, loss = 0.17560109\n",
      "Iteration 1176, loss = 0.17504014\n",
      "Iteration 1177, loss = 0.17448144\n",
      "Iteration 1178, loss = 0.17392496\n",
      "Iteration 1179, loss = 0.17337067\n",
      "Iteration 1180, loss = 0.17281854\n",
      "Iteration 1181, loss = 0.17226854\n",
      "Iteration 1182, loss = 0.17172065\n",
      "Iteration 1183, loss = 0.17117483\n",
      "Iteration 1184, loss = 0.17063106\n",
      "Iteration 1185, loss = 0.17008931\n",
      "Iteration 1186, loss = 0.16954956\n",
      "Iteration 1187, loss = 0.16901179\n",
      "Iteration 1188, loss = 0.16847598\n",
      "Iteration 1189, loss = 0.16794211\n",
      "Iteration 1190, loss = 0.16741016\n",
      "Iteration 1191, loss = 0.16688012\n",
      "Iteration 1192, loss = 0.16635198\n",
      "Iteration 1193, loss = 0.16582573\n",
      "Iteration 1194, loss = 0.16530138\n",
      "Iteration 1195, loss = 0.16477891\n",
      "Iteration 1196, loss = 0.16425833\n",
      "Iteration 1197, loss = 0.16373964\n",
      "Iteration 1198, loss = 0.16322286\n",
      "Iteration 1199, loss = 0.16270799\n",
      "Iteration 1200, loss = 0.16219506\n",
      "Iteration 1201, loss = 0.16168407\n",
      "Iteration 1202, loss = 0.16117504\n",
      "Iteration 1203, loss = 0.16066800\n",
      "Iteration 1204, loss = 0.16016297\n",
      "Iteration 1205, loss = 0.15965999\n",
      "Iteration 1206, loss = 0.15915910\n",
      "Iteration 1207, loss = 0.15866040\n",
      "Iteration 1208, loss = 0.15816412\n",
      "Iteration 1209, loss = 0.15767092\n",
      "Iteration 1210, loss = 0.15718275\n",
      "Iteration 1211, loss = 0.15670508\n",
      "Iteration 1212, loss = 0.15625710\n",
      "Iteration 1213, loss = 0.15588269\n",
      "Iteration 1214, loss = 0.15576355\n",
      "Iteration 1215, loss = 0.15589402\n",
      "Iteration 1216, loss = 0.15679002\n",
      "Iteration 1217, loss = 0.15491328\n",
      "Iteration 1218, loss = 0.15338813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1219, loss = 0.15330497\n",
      "Iteration 1220, loss = 0.15330414\n",
      "Iteration 1221, loss = 0.15250916\n",
      "Iteration 1222, loss = 0.15149500\n",
      "Iteration 1223, loss = 0.15166494\n",
      "Iteration 1224, loss = 0.15143058\n",
      "Iteration 1225, loss = 0.15012792\n",
      "Iteration 1226, loss = 0.15026114\n",
      "Iteration 1227, loss = 0.15010166\n",
      "Iteration 1228, loss = 0.14878185\n",
      "Iteration 1229, loss = 0.14909694\n",
      "Iteration 1230, loss = 0.14873349\n",
      "Iteration 1231, loss = 0.14747147\n",
      "Iteration 1232, loss = 0.14801000\n",
      "Iteration 1233, loss = 0.14728176\n",
      "Iteration 1234, loss = 0.14630302\n",
      "Iteration 1235, loss = 0.14679191\n",
      "Iteration 1236, loss = 0.14560105\n",
      "Iteration 1237, loss = 0.14522782\n",
      "Iteration 1238, loss = 0.14514214\n",
      "Iteration 1239, loss = 0.14403936\n",
      "Iteration 1240, loss = 0.14404857\n",
      "Iteration 1241, loss = 0.14335785\n",
      "Iteration 1242, loss = 0.14289860\n",
      "Iteration 1243, loss = 0.14269844\n",
      "Iteration 1244, loss = 0.14196142\n",
      "Iteration 1245, loss = 0.14185612\n",
      "Iteration 1246, loss = 0.14125664\n",
      "Iteration 1247, loss = 0.14085158\n",
      "Iteration 1248, loss = 0.14055549\n",
      "Iteration 1249, loss = 0.13994252\n",
      "Iteration 1250, loss = 0.13971286\n",
      "Iteration 1251, loss = 0.13918633\n",
      "Iteration 1252, loss = 0.13882374\n",
      "Iteration 1253, loss = 0.13846989\n",
      "Iteration 1254, loss = 0.13797942\n",
      "Iteration 1255, loss = 0.13770053\n",
      "Iteration 1256, loss = 0.13721874\n",
      "Iteration 1257, loss = 0.13686733\n",
      "Iteration 1258, loss = 0.13649105\n",
      "Iteration 1259, loss = 0.13605554\n",
      "Iteration 1260, loss = 0.13573547\n",
      "Iteration 1261, loss = 0.13529996\n",
      "Iteration 1262, loss = 0.13495544\n",
      "Iteration 1263, loss = 0.13457677\n",
      "Iteration 1264, loss = 0.13417964\n",
      "Iteration 1265, loss = 0.13384436\n",
      "Iteration 1266, loss = 0.13343810\n",
      "Iteration 1267, loss = 0.13309114\n",
      "Iteration 1268, loss = 0.13271997\n",
      "Iteration 1269, loss = 0.13234336\n",
      "Iteration 1270, loss = 0.13200064\n",
      "Iteration 1271, loss = 0.13161873\n",
      "Iteration 1272, loss = 0.13127365\n",
      "Iteration 1273, loss = 0.13091223\n",
      "Iteration 1274, loss = 0.13054983\n",
      "Iteration 1275, loss = 0.13020783\n",
      "Iteration 1276, loss = 0.12984232\n",
      "Iteration 1277, loss = 0.12949928\n",
      "Iteration 1278, loss = 0.12914818\n",
      "Iteration 1279, loss = 0.12879601\n",
      "Iteration 1280, loss = 0.12845716\n",
      "Iteration 1281, loss = 0.12810525\n",
      "Iteration 1282, loss = 0.12776662\n",
      "Iteration 1283, loss = 0.12742469\n",
      "Iteration 1284, loss = 0.12708153\n",
      "Iteration 1285, loss = 0.12674781\n",
      "Iteration 1286, loss = 0.12640658\n",
      "Iteration 1287, loss = 0.12607303\n",
      "Iteration 1288, loss = 0.12573956\n",
      "Iteration 1289, loss = 0.12540451\n",
      "Iteration 1290, loss = 0.12507628\n",
      "Iteration 1291, loss = 0.12474448\n",
      "Iteration 1292, loss = 0.12441678\n",
      "Iteration 1293, loss = 0.12409085\n",
      "Iteration 1294, loss = 0.12376364\n",
      "Iteration 1295, loss = 0.12344127\n",
      "Iteration 1296, loss = 0.12311770\n",
      "Iteration 1297, loss = 0.12279620\n",
      "Iteration 1298, loss = 0.12247721\n",
      "Iteration 1299, loss = 0.12215743\n",
      "Iteration 1300, loss = 0.12184093\n",
      "Iteration 1301, loss = 0.12152487\n",
      "Iteration 1302, loss = 0.12120975\n",
      "Iteration 1303, loss = 0.12089716\n",
      "Iteration 1304, loss = 0.12058453\n",
      "Iteration 1305, loss = 0.12027398\n",
      "Iteration 1306, loss = 0.11996473\n",
      "Iteration 1307, loss = 0.11965607\n",
      "Iteration 1308, loss = 0.11934955\n",
      "Iteration 1309, loss = 0.11904367\n",
      "Iteration 1310, loss = 0.11873910\n",
      "Iteration 1311, loss = 0.11843616\n",
      "Iteration 1312, loss = 0.11813386\n",
      "Iteration 1313, loss = 0.11783320\n",
      "Iteration 1314, loss = 0.11753366\n",
      "Iteration 1315, loss = 0.11723508\n",
      "Iteration 1316, loss = 0.11693806\n",
      "Iteration 1317, loss = 0.11664193\n",
      "Iteration 1318, loss = 0.11634703\n",
      "Iteration 1319, loss = 0.11605346\n",
      "Iteration 1320, loss = 0.11576079\n",
      "Iteration 1321, loss = 0.11546945\n",
      "Iteration 1322, loss = 0.11517923\n",
      "Iteration 1323, loss = 0.11489002\n",
      "Iteration 1324, loss = 0.11460210\n",
      "Iteration 1325, loss = 0.11431519\n",
      "Iteration 1326, loss = 0.11402940\n",
      "Iteration 1327, loss = 0.11374480\n",
      "Iteration 1328, loss = 0.11346119\n",
      "Iteration 1329, loss = 0.11317873\n",
      "Iteration 1330, loss = 0.11289738\n",
      "Iteration 1331, loss = 0.11261703\n",
      "Iteration 1332, loss = 0.11233783\n",
      "Iteration 1333, loss = 0.11205968\n",
      "Iteration 1334, loss = 0.11178256\n",
      "Iteration 1335, loss = 0.11150655\n",
      "Iteration 1336, loss = 0.11123156\n",
      "Iteration 1337, loss = 0.11095762\n",
      "Iteration 1338, loss = 0.11068474\n",
      "Iteration 1339, loss = 0.11041288\n",
      "Iteration 1340, loss = 0.11014206\n",
      "Iteration 1341, loss = 0.10987228\n",
      "Iteration 1342, loss = 0.10960350\n",
      "Iteration 1343, loss = 0.10933576\n",
      "Iteration 1344, loss = 0.10906904\n",
      "Iteration 1345, loss = 0.10880332\n",
      "Iteration 1346, loss = 0.10853862\n",
      "Iteration 1347, loss = 0.10827492\n",
      "Iteration 1348, loss = 0.10801222\n",
      "Iteration 1349, loss = 0.10775053\n",
      "Iteration 1350, loss = 0.10748983\n",
      "Iteration 1351, loss = 0.10723012\n",
      "Iteration 1352, loss = 0.10697140\n",
      "Iteration 1353, loss = 0.10671367\n",
      "Iteration 1354, loss = 0.10645692\n",
      "Iteration 1355, loss = 0.10620115\n",
      "Iteration 1356, loss = 0.10594636\n",
      "Iteration 1357, loss = 0.10569255\n",
      "Iteration 1358, loss = 0.10543971\n",
      "Iteration 1359, loss = 0.10518784\n",
      "Iteration 1360, loss = 0.10493694\n",
      "Iteration 1361, loss = 0.10468700\n",
      "Iteration 1362, loss = 0.10443803\n",
      "Iteration 1363, loss = 0.10419002\n",
      "Iteration 1364, loss = 0.10394297\n",
      "Iteration 1365, loss = 0.10369687\n",
      "Iteration 1366, loss = 0.10345173\n",
      "Iteration 1367, loss = 0.10320754\n",
      "Iteration 1368, loss = 0.10296430\n",
      "Iteration 1369, loss = 0.10272201\n",
      "Iteration 1370, loss = 0.10248067\n",
      "Iteration 1371, loss = 0.10224027\n",
      "Iteration 1372, loss = 0.10200081\n",
      "Iteration 1373, loss = 0.10176229\n",
      "Iteration 1374, loss = 0.10152471\n",
      "Iteration 1375, loss = 0.10128806\n",
      "Iteration 1376, loss = 0.10105235\n",
      "Iteration 1377, loss = 0.10081756\n",
      "Iteration 1378, loss = 0.10058371\n",
      "Iteration 1379, loss = 0.10035078\n",
      "Iteration 1380, loss = 0.10011877\n",
      "Iteration 1381, loss = 0.09988769\n",
      "Iteration 1382, loss = 0.09965752\n",
      "Iteration 1383, loss = 0.09942827\n",
      "Iteration 1384, loss = 0.09919994\n",
      "Iteration 1385, loss = 0.09897251\n",
      "Iteration 1386, loss = 0.09874600\n",
      "Iteration 1387, loss = 0.09852039\n",
      "Iteration 1388, loss = 0.09829568\n",
      "Iteration 1389, loss = 0.09807188\n",
      "Iteration 1390, loss = 0.09784897\n",
      "Iteration 1391, loss = 0.09762696\n",
      "Iteration 1392, loss = 0.09740584\n",
      "Iteration 1393, loss = 0.09718561\n",
      "Iteration 1394, loss = 0.09696627\n",
      "Iteration 1395, loss = 0.09674781\n",
      "Iteration 1396, loss = 0.09653023\n",
      "Iteration 1397, loss = 0.09631353\n",
      "Iteration 1398, loss = 0.09609770\n",
      "Iteration 1399, loss = 0.09588275\n",
      "Iteration 1400, loss = 0.09566866\n",
      "Iteration 1401, loss = 0.09545544\n",
      "Iteration 1402, loss = 0.09524308\n",
      "Iteration 1403, loss = 0.09503158\n",
      "Iteration 1404, loss = 0.09482094\n",
      "Iteration 1405, loss = 0.09461115\n",
      "Iteration 1406, loss = 0.09440220\n",
      "Iteration 1407, loss = 0.09419411\n",
      "Iteration 1408, loss = 0.09398685\n",
      "Iteration 1409, loss = 0.09378043\n",
      "Iteration 1410, loss = 0.09357485\n",
      "Iteration 1411, loss = 0.09337009\n",
      "Iteration 1412, loss = 0.09316617\n",
      "Iteration 1413, loss = 0.09296307\n",
      "Iteration 1414, loss = 0.09276079\n",
      "Iteration 1415, loss = 0.09255933\n",
      "Iteration 1416, loss = 0.09235868\n",
      "Iteration 1417, loss = 0.09215883\n",
      "Iteration 1418, loss = 0.09195980\n",
      "Iteration 1419, loss = 0.09176157\n",
      "Iteration 1420, loss = 0.09156413\n",
      "Iteration 1421, loss = 0.09136750\n",
      "Iteration 1422, loss = 0.09117165\n",
      "Iteration 1423, loss = 0.09097659\n",
      "Iteration 1424, loss = 0.09078231\n",
      "Iteration 1425, loss = 0.09058881\n",
      "Iteration 1426, loss = 0.09039609\n",
      "Iteration 1427, loss = 0.09020414\n",
      "Iteration 1428, loss = 0.09001296\n",
      "Iteration 1429, loss = 0.08982254\n",
      "Iteration 1430, loss = 0.08963288\n",
      "Iteration 1431, loss = 0.08944398\n",
      "Iteration 1432, loss = 0.08925584\n",
      "Iteration 1433, loss = 0.08906844\n",
      "Iteration 1434, loss = 0.08888179\n",
      "Iteration 1435, loss = 0.08869588\n",
      "Iteration 1436, loss = 0.08851071\n",
      "Iteration 1437, loss = 0.08832627\n",
      "Iteration 1438, loss = 0.08814256\n",
      "Iteration 1439, loss = 0.08795958\n",
      "Iteration 1440, loss = 0.08777732\n",
      "Iteration 1441, loss = 0.08759578\n",
      "Iteration 1442, loss = 0.08741495\n",
      "Iteration 1443, loss = 0.08723484\n",
      "Iteration 1444, loss = 0.08705543\n",
      "Iteration 1445, loss = 0.08687672\n",
      "Iteration 1446, loss = 0.08669872\n",
      "Iteration 1447, loss = 0.08652141\n",
      "Iteration 1448, loss = 0.08634480\n",
      "Iteration 1449, loss = 0.08616887\n",
      "Iteration 1450, loss = 0.08599363\n",
      "Iteration 1451, loss = 0.08581907\n",
      "Iteration 1452, loss = 0.08564518\n",
      "Iteration 1453, loss = 0.08547197\n",
      "Iteration 1454, loss = 0.08529943\n",
      "Iteration 1455, loss = 0.08512756\n",
      "Iteration 1456, loss = 0.08495635\n",
      "Iteration 1457, loss = 0.08478580\n",
      "Iteration 1458, loss = 0.08461590\n",
      "Iteration 1459, loss = 0.08444665\n",
      "Iteration 1460, loss = 0.08427806\n",
      "Iteration 1461, loss = 0.08411011\n",
      "Iteration 1462, loss = 0.08394279\n",
      "Iteration 1463, loss = 0.08377612\n",
      "Iteration 1464, loss = 0.08361008\n",
      "Iteration 1465, loss = 0.08344467\n",
      "Iteration 1466, loss = 0.08327989\n",
      "Iteration 1467, loss = 0.08311573\n",
      "Iteration 1468, loss = 0.08295219\n",
      "Iteration 1469, loss = 0.08278927\n",
      "Iteration 1470, loss = 0.08262696\n",
      "Iteration 1471, loss = 0.08246526\n",
      "Iteration 1472, loss = 0.08230416\n",
      "Iteration 1473, loss = 0.08214367\n",
      "Iteration 1474, loss = 0.08198378\n",
      "Iteration 1475, loss = 0.08182449\n",
      "Iteration 1476, loss = 0.08166578\n",
      "Iteration 1477, loss = 0.08150767\n",
      "Iteration 1478, loss = 0.08135014\n",
      "Iteration 1479, loss = 0.08119319\n",
      "Iteration 1480, loss = 0.08103683\n",
      "Iteration 1481, loss = 0.08088104\n",
      "Iteration 1482, loss = 0.08072582\n",
      "Iteration 1483, loss = 0.08057117\n",
      "Iteration 1484, loss = 0.08041709\n",
      "Iteration 1485, loss = 0.08026357\n",
      "Iteration 1486, loss = 0.08011061\n",
      "Iteration 1487, loss = 0.07995821\n",
      "Iteration 1488, loss = 0.07980636\n",
      "Iteration 1489, loss = 0.07965506\n",
      "Iteration 1490, loss = 0.07950431\n",
      "Iteration 1491, loss = 0.07935411\n",
      "Iteration 1492, loss = 0.07920444\n",
      "Iteration 1493, loss = 0.07905532\n",
      "Iteration 1494, loss = 0.07890673\n",
      "Iteration 1495, loss = 0.07875867\n",
      "Iteration 1496, loss = 0.07861114\n",
      "Iteration 1497, loss = 0.07846413\n",
      "Iteration 1498, loss = 0.07831765\n",
      "Iteration 1499, loss = 0.07817169\n",
      "Iteration 1500, loss = 0.07802625\n",
      "Iteration 1501, loss = 0.07788132\n",
      "Iteration 1502, loss = 0.07773690\n",
      "Iteration 1503, loss = 0.07759299\n",
      "Iteration 1504, loss = 0.07744958\n",
      "Iteration 1505, loss = 0.07730668\n",
      "Iteration 1506, loss = 0.07716428\n",
      "Iteration 1507, loss = 0.07702238\n",
      "Iteration 1508, loss = 0.07688097\n",
      "Iteration 1509, loss = 0.07674005\n",
      "Iteration 1510, loss = 0.07659962\n",
      "Iteration 1511, loss = 0.07645967\n",
      "Iteration 1512, loss = 0.07632021\n",
      "Iteration 1513, loss = 0.07618123\n",
      "Iteration 1514, loss = 0.07604272\n",
      "Iteration 1515, loss = 0.07590470\n",
      "Iteration 1516, loss = 0.07576714\n",
      "Iteration 1517, loss = 0.07563005\n",
      "Iteration 1518, loss = 0.07549343\n",
      "Iteration 1519, loss = 0.07535728\n",
      "Iteration 1520, loss = 0.07522158\n",
      "Iteration 1521, loss = 0.07508635\n",
      "Iteration 1522, loss = 0.07495157\n",
      "Iteration 1523, loss = 0.07481725\n",
      "Iteration 1524, loss = 0.07468337\n",
      "Iteration 1525, loss = 0.07454995\n",
      "Iteration 1526, loss = 0.07441697\n",
      "Iteration 1527, loss = 0.07428444\n",
      "Iteration 1528, loss = 0.07415234\n",
      "Iteration 1529, loss = 0.07402069\n",
      "Iteration 1530, loss = 0.07388947\n",
      "Iteration 1531, loss = 0.07375869\n",
      "Iteration 1532, loss = 0.07362833\n",
      "Iteration 1533, loss = 0.07349841\n",
      "Iteration 1534, loss = 0.07336891\n",
      "Iteration 1535, loss = 0.07323984\n",
      "Iteration 1536, loss = 0.07311119\n",
      "Iteration 1537, loss = 0.07298296\n",
      "Iteration 1538, loss = 0.07285514\n",
      "Iteration 1539, loss = 0.07272774\n",
      "Iteration 1540, loss = 0.07260075\n",
      "Iteration 1541, loss = 0.07247418\n",
      "Iteration 1542, loss = 0.07234801\n",
      "Iteration 1543, loss = 0.07222224\n",
      "Iteration 1544, loss = 0.07209688\n",
      "Iteration 1545, loss = 0.07197192\n",
      "Iteration 1546, loss = 0.07184736\n",
      "Iteration 1547, loss = 0.07172320\n",
      "Iteration 1548, loss = 0.07159942\n",
      "Iteration 1549, loss = 0.07147605\n",
      "Iteration 1550, loss = 0.07135306\n",
      "Iteration 1551, loss = 0.07123046\n",
      "Iteration 1552, loss = 0.07110824\n",
      "Iteration 1553, loss = 0.07098641\n",
      "Iteration 1554, loss = 0.07086496\n",
      "Iteration 1555, loss = 0.07074389\n",
      "Iteration 1556, loss = 0.07062319\n",
      "Iteration 1557, loss = 0.07050287\n",
      "Iteration 1558, loss = 0.07038293\n",
      "Iteration 1559, loss = 0.07026335\n",
      "Iteration 1560, loss = 0.07014414\n",
      "Iteration 1561, loss = 0.07002530\n",
      "Iteration 1562, loss = 0.06990683\n",
      "Iteration 1563, loss = 0.06978872\n",
      "Iteration 1564, loss = 0.06967096\n",
      "Iteration 1565, loss = 0.06955357\n",
      "Iteration 1566, loss = 0.06943653\n",
      "Iteration 1567, loss = 0.06931985\n",
      "Iteration 1568, loss = 0.06920352\n",
      "Iteration 1569, loss = 0.06908754\n",
      "Iteration 1570, loss = 0.06897191\n",
      "Iteration 1571, loss = 0.06885663\n",
      "Iteration 1572, loss = 0.06874169\n",
      "Iteration 1573, loss = 0.06862710\n",
      "Iteration 1574, loss = 0.06851285\n",
      "Iteration 1575, loss = 0.06839893\n",
      "Iteration 1576, loss = 0.06828536\n",
      "Iteration 1577, loss = 0.06817212\n",
      "Iteration 1578, loss = 0.06805921\n",
      "Iteration 1579, loss = 0.06794664\n",
      "Iteration 1580, loss = 0.06783439\n",
      "Iteration 1581, loss = 0.06772248\n",
      "Iteration 1582, loss = 0.06761089\n",
      "Iteration 1583, loss = 0.06749963\n",
      "Iteration 1584, loss = 0.06738869\n",
      "Iteration 1585, loss = 0.06727807\n",
      "Iteration 1586, loss = 0.06716778\n",
      "Iteration 1587, loss = 0.06705780\n",
      "Iteration 1588, loss = 0.06694814\n",
      "Iteration 1589, loss = 0.06683879\n",
      "Iteration 1590, loss = 0.06672975\n",
      "Iteration 1591, loss = 0.06662103\n",
      "Iteration 1592, loss = 0.06651262\n",
      "Iteration 1593, loss = 0.06640451\n",
      "Iteration 1594, loss = 0.06629672\n",
      "Iteration 1595, loss = 0.06618922\n",
      "Iteration 1596, loss = 0.06608204\n",
      "Iteration 1597, loss = 0.06597515\n",
      "Iteration 1598, loss = 0.06586856\n",
      "Iteration 1599, loss = 0.06576228\n",
      "Iteration 1600, loss = 0.06565629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1601, loss = 0.06555059\n",
      "Iteration 1602, loss = 0.06544519\n",
      "Iteration 1603, loss = 0.06534009\n",
      "Iteration 1604, loss = 0.06523527\n",
      "Iteration 1605, loss = 0.06513075\n",
      "Iteration 1606, loss = 0.06502652\n",
      "Iteration 1607, loss = 0.06492257\n",
      "Iteration 1608, loss = 0.06481891\n",
      "Iteration 1609, loss = 0.06471553\n",
      "Iteration 1610, loss = 0.06461244\n",
      "Iteration 1611, loss = 0.06450962\n",
      "Iteration 1612, loss = 0.06440709\n",
      "Iteration 1613, loss = 0.06430484\n",
      "Iteration 1614, loss = 0.06420287\n",
      "Iteration 1615, loss = 0.06410117\n",
      "Iteration 1616, loss = 0.06399975\n",
      "Iteration 1617, loss = 0.06389860\n",
      "Iteration 1618, loss = 0.06379772\n",
      "Iteration 1619, loss = 0.06369712\n",
      "Iteration 1620, loss = 0.06359679\n",
      "Iteration 1621, loss = 0.06349672\n",
      "Iteration 1622, loss = 0.06339693\n",
      "Iteration 1623, loss = 0.06329740\n",
      "Iteration 1624, loss = 0.06319814\n",
      "Iteration 1625, loss = 0.06309914\n",
      "Iteration 1626, loss = 0.06300041\n",
      "Iteration 1627, loss = 0.06290193\n",
      "Iteration 1628, loss = 0.06280372\n",
      "Iteration 1629, loss = 0.06270577\n",
      "Iteration 1630, loss = 0.06260808\n",
      "Iteration 1631, loss = 0.06251065\n",
      "Iteration 1632, loss = 0.06241348\n",
      "Iteration 1633, loss = 0.06231656\n",
      "Iteration 1634, loss = 0.06221990\n",
      "Iteration 1635, loss = 0.06212349\n",
      "Iteration 1636, loss = 0.06202733\n",
      "Iteration 1637, loss = 0.06193143\n",
      "Iteration 1638, loss = 0.06183578\n",
      "Iteration 1639, loss = 0.06174038\n",
      "Iteration 1640, loss = 0.06164523\n",
      "Iteration 1641, loss = 0.06155033\n",
      "Iteration 1642, loss = 0.06145567\n",
      "Iteration 1643, loss = 0.06136127\n",
      "Iteration 1644, loss = 0.06126711\n",
      "Iteration 1645, loss = 0.06117319\n",
      "Iteration 1646, loss = 0.06107952\n",
      "Iteration 1647, loss = 0.06098609\n",
      "Iteration 1648, loss = 0.06089291\n",
      "Iteration 1649, loss = 0.06079997\n",
      "Iteration 1650, loss = 0.06070729\n",
      "Iteration 1651, loss = 0.06061485\n",
      "Iteration 1652, loss = 0.06052270\n",
      "Iteration 1653, loss = 0.06043086\n",
      "Iteration 1654, loss = 0.06033948\n",
      "Iteration 1655, loss = 0.06024884\n",
      "Iteration 1656, loss = 0.06015971\n",
      "Iteration 1657, loss = 0.06007380\n",
      "Iteration 1658, loss = 0.05999614\n",
      "Iteration 1659, loss = 0.05993609\n",
      "Iteration 1660, loss = 0.05992795\n",
      "Iteration 1661, loss = 0.06000380\n",
      "Iteration 1662, loss = 0.06036978\n",
      "Iteration 1663, loss = 0.06072206\n",
      "Iteration 1664, loss = 0.06155459\n",
      "Iteration 1665, loss = 0.06033601\n",
      "Iteration 1666, loss = 0.05937966\n",
      "Iteration 1667, loss = 0.05937730\n",
      "Iteration 1668, loss = 0.05994862\n",
      "Iteration 1669, loss = 0.06027310\n",
      "Iteration 1670, loss = 0.05916435\n",
      "Iteration 1671, loss = 0.05896478\n",
      "Iteration 1672, loss = 0.05953006\n",
      "Iteration 1673, loss = 0.05904840\n",
      "Iteration 1674, loss = 0.05857829\n",
      "Iteration 1675, loss = 0.05876790\n",
      "Iteration 1676, loss = 0.05877428\n",
      "Iteration 1677, loss = 0.05843426\n",
      "Iteration 1678, loss = 0.05826571\n",
      "Iteration 1679, loss = 0.05839603\n",
      "Iteration 1680, loss = 0.05828645\n",
      "Iteration 1681, loss = 0.05798881\n",
      "Iteration 1682, loss = 0.05804166\n",
      "Iteration 1683, loss = 0.05806932\n",
      "Iteration 1684, loss = 0.05777434\n",
      "Iteration 1685, loss = 0.05770619\n",
      "Iteration 1686, loss = 0.05775973\n",
      "Iteration 1687, loss = 0.05754995\n",
      "Iteration 1688, loss = 0.05742030\n",
      "Iteration 1689, loss = 0.05744311\n",
      "Iteration 1690, loss = 0.05731712\n",
      "Iteration 1691, loss = 0.05716427\n",
      "Iteration 1692, loss = 0.05713477\n",
      "Iteration 1693, loss = 0.05706715\n",
      "Iteration 1694, loss = 0.05693013\n",
      "Iteration 1695, loss = 0.05685714\n",
      "Iteration 1696, loss = 0.05681041\n",
      "Iteration 1697, loss = 0.05670197\n",
      "Iteration 1698, loss = 0.05660247\n",
      "Iteration 1699, loss = 0.05655236\n",
      "Iteration 1700, loss = 0.05647092\n",
      "Iteration 1701, loss = 0.05636370\n",
      "Iteration 1702, loss = 0.05629841\n",
      "Iteration 1703, loss = 0.05623250\n",
      "Iteration 1704, loss = 0.05613194\n",
      "Iteration 1705, loss = 0.05605170\n",
      "Iteration 1706, loss = 0.05598932\n",
      "Iteration 1707, loss = 0.05590167\n",
      "Iteration 1708, loss = 0.05581368\n",
      "Iteration 1709, loss = 0.05574634\n",
      "Iteration 1710, loss = 0.05566991\n",
      "Iteration 1711, loss = 0.05558239\n",
      "Iteration 1712, loss = 0.05550742\n",
      "Iteration 1713, loss = 0.05543657\n",
      "Iteration 1714, loss = 0.05535463\n",
      "Iteration 1715, loss = 0.05527434\n",
      "Iteration 1716, loss = 0.05520314\n",
      "Iteration 1717, loss = 0.05512720\n",
      "Iteration 1718, loss = 0.05504617\n",
      "Iteration 1719, loss = 0.05497188\n",
      "Iteration 1720, loss = 0.05489931\n",
      "Iteration 1721, loss = 0.05482088\n",
      "Iteration 1722, loss = 0.05474401\n",
      "Iteration 1723, loss = 0.05467159\n",
      "Iteration 1724, loss = 0.05459662\n",
      "Iteration 1725, loss = 0.05451957\n",
      "Iteration 1726, loss = 0.05444572\n",
      "Iteration 1727, loss = 0.05437284\n",
      "Iteration 1728, loss = 0.05429747\n",
      "Iteration 1729, loss = 0.05422255\n",
      "Iteration 1730, loss = 0.05414995\n",
      "Iteration 1731, loss = 0.05407657\n",
      "Iteration 1732, loss = 0.05400196\n",
      "Iteration 1733, loss = 0.05392881\n",
      "Iteration 1734, loss = 0.05385654\n",
      "Iteration 1735, loss = 0.05378319\n",
      "Iteration 1736, loss = 0.05370986\n",
      "Iteration 1737, loss = 0.05363776\n",
      "Iteration 1738, loss = 0.05356565\n",
      "Iteration 1739, loss = 0.05349292\n",
      "Iteration 1740, loss = 0.05342074\n",
      "Iteration 1741, loss = 0.05334927\n",
      "Iteration 1742, loss = 0.05327752\n",
      "Iteration 1743, loss = 0.05320563\n",
      "Iteration 1744, loss = 0.05313435\n",
      "Iteration 1745, loss = 0.05306338\n",
      "Iteration 1746, loss = 0.05299218\n",
      "Iteration 1747, loss = 0.05292114\n",
      "Iteration 1748, loss = 0.05285058\n",
      "Iteration 1749, loss = 0.05278013\n",
      "Iteration 1750, loss = 0.05270959\n",
      "Iteration 1751, loss = 0.05263932\n",
      "Iteration 1752, loss = 0.05256939\n",
      "Iteration 1753, loss = 0.05249951\n",
      "Iteration 1754, loss = 0.05242966\n",
      "Iteration 1755, loss = 0.05236010\n",
      "Iteration 1756, loss = 0.05229077\n",
      "Iteration 1757, loss = 0.05222148\n",
      "Iteration 1758, loss = 0.05215232\n",
      "Iteration 1759, loss = 0.05208341\n",
      "Iteration 1760, loss = 0.05201468\n",
      "Iteration 1761, loss = 0.05194602\n",
      "Iteration 1762, loss = 0.05187751\n",
      "Iteration 1763, loss = 0.05180922\n",
      "Iteration 1764, loss = 0.05174109\n",
      "Iteration 1765, loss = 0.05167305\n",
      "Iteration 1766, loss = 0.05160518\n",
      "Iteration 1767, loss = 0.05153750\n",
      "Iteration 1768, loss = 0.05146996\n",
      "Iteration 1769, loss = 0.05140254\n",
      "Iteration 1770, loss = 0.05133529\n",
      "Iteration 1771, loss = 0.05126821\n",
      "Iteration 1772, loss = 0.05120126\n",
      "Iteration 1773, loss = 0.05113444\n",
      "Iteration 1774, loss = 0.05106779\n",
      "Iteration 1775, loss = 0.05100130\n",
      "Iteration 1776, loss = 0.05093494\n",
      "Iteration 1777, loss = 0.05086872\n",
      "Iteration 1778, loss = 0.05080265\n",
      "Iteration 1779, loss = 0.05073674\n",
      "Iteration 1780, loss = 0.05067096\n",
      "Iteration 1781, loss = 0.05060532\n",
      "Iteration 1782, loss = 0.05053983\n",
      "Iteration 1783, loss = 0.05047449\n",
      "Iteration 1784, loss = 0.05040929\n",
      "Iteration 1785, loss = 0.05034422\n",
      "Iteration 1786, loss = 0.05027930\n",
      "Iteration 1787, loss = 0.05021452\n",
      "Iteration 1788, loss = 0.05014988\n",
      "Iteration 1789, loss = 0.05008537\n",
      "Iteration 1790, loss = 0.05002101\n",
      "Iteration 1791, loss = 0.04995679\n",
      "Iteration 1792, loss = 0.04989270\n",
      "Iteration 1793, loss = 0.04982874\n",
      "Iteration 1794, loss = 0.04976493\n",
      "Iteration 1795, loss = 0.04970125\n",
      "Iteration 1796, loss = 0.04963771\n",
      "Iteration 1797, loss = 0.04957430\n",
      "Iteration 1798, loss = 0.04951103\n",
      "Iteration 1799, loss = 0.04944789\n",
      "Iteration 1800, loss = 0.04938488\n",
      "Iteration 1801, loss = 0.04932200\n",
      "Iteration 1802, loss = 0.04925926\n",
      "Iteration 1803, loss = 0.04919665\n",
      "Iteration 1804, loss = 0.04913417\n",
      "Iteration 1805, loss = 0.04907182\n",
      "Iteration 1806, loss = 0.04900960\n",
      "Iteration 1807, loss = 0.04894751\n",
      "Iteration 1808, loss = 0.04888555\n",
      "Iteration 1809, loss = 0.04882372\n",
      "Iteration 1810, loss = 0.04876201\n",
      "Iteration 1811, loss = 0.04870043\n",
      "Iteration 1812, loss = 0.04863898\n",
      "Iteration 1813, loss = 0.04857766\n",
      "Iteration 1814, loss = 0.04851646\n",
      "Iteration 1815, loss = 0.04845539\n",
      "Iteration 1816, loss = 0.04839444\n",
      "Iteration 1817, loss = 0.04833361\n",
      "Iteration 1818, loss = 0.04827291\n",
      "Iteration 1819, loss = 0.04821234\n",
      "Iteration 1820, loss = 0.04815188\n",
      "Iteration 1821, loss = 0.04809155\n",
      "Iteration 1822, loss = 0.04803134\n",
      "Iteration 1823, loss = 0.04797125\n",
      "Iteration 1824, loss = 0.04791128\n",
      "Iteration 1825, loss = 0.04785144\n",
      "Iteration 1826, loss = 0.04779171\n",
      "Iteration 1827, loss = 0.04773210\n",
      "Iteration 1828, loss = 0.04767261\n",
      "Iteration 1829, loss = 0.04761324\n",
      "Iteration 1830, loss = 0.04755399\n",
      "Iteration 1831, loss = 0.04749486\n",
      "Iteration 1832, loss = 0.04743584\n",
      "Iteration 1833, loss = 0.04737694\n",
      "Iteration 1834, loss = 0.04731815\n",
      "Iteration 1835, loss = 0.04725948\n",
      "Iteration 1836, loss = 0.04720093\n",
      "Iteration 1837, loss = 0.04714249\n",
      "Iteration 1838, loss = 0.04708417\n",
      "Iteration 1839, loss = 0.04702596\n",
      "Iteration 1840, loss = 0.04696786\n",
      "Iteration 1841, loss = 0.04690988\n",
      "Iteration 1842, loss = 0.04685200\n",
      "Iteration 1843, loss = 0.04679425\n",
      "Iteration 1844, loss = 0.04673660\n",
      "Iteration 1845, loss = 0.04667906\n",
      "Iteration 1846, loss = 0.04662164\n",
      "Iteration 1847, loss = 0.04656432\n",
      "Iteration 1848, loss = 0.04650711\n",
      "Iteration 1849, loss = 0.04645002\n",
      "Iteration 1850, loss = 0.04639303\n",
      "Iteration 1851, loss = 0.04633615\n",
      "Iteration 1852, loss = 0.04627938\n",
      "Iteration 1853, loss = 0.04622272\n",
      "Iteration 1854, loss = 0.04616617\n",
      "Iteration 1855, loss = 0.04610972\n",
      "Iteration 1856, loss = 0.04605338\n",
      "Iteration 1857, loss = 0.04599714\n",
      "Iteration 1858, loss = 0.04594101\n",
      "Iteration 1859, loss = 0.04588499\n",
      "Iteration 1860, loss = 0.04582907\n",
      "Iteration 1861, loss = 0.04577326\n",
      "Iteration 1862, loss = 0.04571754\n",
      "Iteration 1863, loss = 0.04566194\n",
      "Iteration 1864, loss = 0.04560643\n",
      "Iteration 1865, loss = 0.04555103\n",
      "Iteration 1866, loss = 0.04549574\n",
      "Iteration 1867, loss = 0.04544054\n",
      "Iteration 1868, loss = 0.04538545\n",
      "Iteration 1869, loss = 0.04533045\n",
      "Iteration 1870, loss = 0.04527556\n",
      "Iteration 1871, loss = 0.04522077\n",
      "Iteration 1872, loss = 0.04516608\n",
      "Iteration 1873, loss = 0.04511149\n",
      "Iteration 1874, loss = 0.04505699\n",
      "Iteration 1875, loss = 0.04500260\n",
      "Iteration 1876, loss = 0.04494830\n",
      "Iteration 1877, loss = 0.04489411\n",
      "Iteration 1878, loss = 0.04484001\n",
      "Iteration 1879, loss = 0.04478601\n",
      "Iteration 1880, loss = 0.04473210\n",
      "Iteration 1881, loss = 0.04467830\n",
      "Iteration 1882, loss = 0.04462459\n",
      "Iteration 1883, loss = 0.04457097\n",
      "Iteration 1884, loss = 0.04451745\n",
      "Iteration 1885, loss = 0.04446403\n",
      "Iteration 1886, loss = 0.04441070\n",
      "Iteration 1887, loss = 0.04435747\n",
      "Iteration 1888, loss = 0.04430433\n",
      "Iteration 1889, loss = 0.04425128\n",
      "Iteration 1890, loss = 0.04419833\n",
      "Iteration 1891, loss = 0.04414547\n",
      "Iteration 1892, loss = 0.04409270\n",
      "Iteration 1893, loss = 0.04404003\n",
      "Iteration 1894, loss = 0.04398744\n",
      "Iteration 1895, loss = 0.04393495\n",
      "Iteration 1896, loss = 0.04388256\n",
      "Iteration 1897, loss = 0.04383025\n",
      "Iteration 1898, loss = 0.04377803\n",
      "Iteration 1899, loss = 0.04372590\n",
      "Iteration 1900, loss = 0.04367387\n",
      "Iteration 1901, loss = 0.04362192\n",
      "Iteration 1902, loss = 0.04357006\n",
      "Iteration 1903, loss = 0.04351829\n",
      "Iteration 1904, loss = 0.04346661\n",
      "Iteration 1905, loss = 0.04341502\n",
      "Iteration 1906, loss = 0.04336352\n",
      "Iteration 1907, loss = 0.04331210\n",
      "Iteration 1908, loss = 0.04326077\n",
      "Iteration 1909, loss = 0.04320953\n",
      "Iteration 1910, loss = 0.04315838\n",
      "Iteration 1911, loss = 0.04310731\n",
      "Iteration 1912, loss = 0.04305633\n",
      "Iteration 1913, loss = 0.04300543\n",
      "Iteration 1914, loss = 0.04295462\n",
      "Iteration 1915, loss = 0.04290390\n",
      "Iteration 1916, loss = 0.04285325\n",
      "Iteration 1917, loss = 0.04280270\n",
      "Iteration 1918, loss = 0.04275223\n",
      "Iteration 1919, loss = 0.04270184\n",
      "Iteration 1920, loss = 0.04265153\n",
      "Iteration 1921, loss = 0.04260131\n",
      "Iteration 1922, loss = 0.04255118\n",
      "Iteration 1923, loss = 0.04250112\n",
      "Iteration 1924, loss = 0.04245115\n",
      "Iteration 1925, loss = 0.04240126\n",
      "Iteration 1926, loss = 0.04235145\n",
      "Iteration 1927, loss = 0.04230172\n",
      "Iteration 1928, loss = 0.04225207\n",
      "Iteration 1929, loss = 0.04220251\n",
      "Iteration 1930, loss = 0.04215302\n",
      "Iteration 1931, loss = 0.04210362\n",
      "Iteration 1932, loss = 0.04205429\n",
      "Iteration 1933, loss = 0.04200505\n",
      "Iteration 1934, loss = 0.04195588\n",
      "Iteration 1935, loss = 0.04190680\n",
      "Iteration 1936, loss = 0.04185779\n",
      "Iteration 1937, loss = 0.04180886\n",
      "Iteration 1938, loss = 0.04176001\n",
      "Iteration 1939, loss = 0.04171124\n",
      "Iteration 1940, loss = 0.04166254\n",
      "Iteration 1941, loss = 0.04161392\n",
      "Iteration 1942, loss = 0.04156538\n",
      "Iteration 1943, loss = 0.04151692\n",
      "Iteration 1944, loss = 0.04146853\n",
      "Iteration 1945, loss = 0.04142022\n",
      "Iteration 1946, loss = 0.04137199\n",
      "Iteration 1947, loss = 0.04132383\n",
      "Iteration 1948, loss = 0.04127575\n",
      "Iteration 1949, loss = 0.04122774\n",
      "Iteration 1950, loss = 0.04117981\n",
      "Iteration 1951, loss = 0.04113195\n",
      "Iteration 1952, loss = 0.04108417\n",
      "Iteration 1953, loss = 0.04103646\n",
      "Iteration 1954, loss = 0.04098882\n",
      "Iteration 1955, loss = 0.04094126\n",
      "Iteration 1956, loss = 0.04089377\n",
      "Iteration 1957, loss = 0.04084636\n",
      "Iteration 1958, loss = 0.04079901\n",
      "Iteration 1959, loss = 0.04075174\n",
      "Iteration 1960, loss = 0.04070455\n",
      "Iteration 1961, loss = 0.04065742\n",
      "Iteration 1962, loss = 0.04061037\n",
      "Iteration 1963, loss = 0.04056339\n",
      "Iteration 1964, loss = 0.04051648\n",
      "Iteration 1965, loss = 0.04046964\n",
      "Iteration 1966, loss = 0.04042287\n",
      "Iteration 1967, loss = 0.04037617\n",
      "Iteration 1968, loss = 0.04032954\n",
      "Iteration 1969, loss = 0.04028298\n",
      "Iteration 1970, loss = 0.04023650\n",
      "Iteration 1971, loss = 0.04019008\n",
      "Iteration 1972, loss = 0.04014373\n",
      "Iteration 1973, loss = 0.04009745\n",
      "Iteration 1974, loss = 0.04005124\n",
      "Iteration 1975, loss = 0.04000510\n",
      "Iteration 1976, loss = 0.03995902\n",
      "Iteration 1977, loss = 0.03991302\n",
      "Iteration 1978, loss = 0.03986708\n",
      "Iteration 1979, loss = 0.03982121\n",
      "Iteration 1980, loss = 0.03977541\n",
      "Iteration 1981, loss = 0.03972967\n",
      "Iteration 1982, loss = 0.03968401\n",
      "Iteration 1983, loss = 0.03963841\n",
      "Iteration 1984, loss = 0.03959287\n",
      "Iteration 1985, loss = 0.03954740\n",
      "Iteration 1986, loss = 0.03950200\n",
      "Iteration 1987, loss = 0.03945666\n",
      "Iteration 1988, loss = 0.03941139\n",
      "Iteration 1989, loss = 0.03936619\n",
      "Iteration 1990, loss = 0.03932105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1991, loss = 0.03927597\n",
      "Iteration 1992, loss = 0.03923096\n",
      "Iteration 1993, loss = 0.03918601\n",
      "Iteration 1994, loss = 0.03914113\n",
      "Iteration 1995, loss = 0.03909631\n",
      "Iteration 1996, loss = 0.03905156\n",
      "Iteration 1997, loss = 0.03900687\n",
      "Iteration 1998, loss = 0.03896224\n",
      "Iteration 1999, loss = 0.03891767\n",
      "Iteration 2000, loss = 0.03887317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(8, 8), max_iter=2000,\n",
       "              tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13 -> 8 -> 8 -> 3\n",
    "network = MLPClassifier(max_iter=2000,\n",
    "                        verbose=True,\n",
    "                        activation = 'logistic',\n",
    "                        solver = 'adam',\n",
    "                        learning_rate = 'constant',\n",
    "                        learning_rate_init = 0.001,\n",
    "                        tol=0.00000100,\n",
    "                        hidden_layer_sizes = (8, 8))\n",
    "network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "adSet8VkNt4U",
    "outputId": "fe6153da-025d-46c5-d929-10b418f52f89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0c9uGEYZNxy3",
    "outputId": "5f4d92a1-1622-4bb0-dcd9-bfc26bdac740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 8.26202545e-01, -9.98347316e-03, -5.98005690e-01,\n",
       "         -9.56205289e-02,  5.08713945e-01,  3.56892390e-02,\n",
       "         -1.19315098e-01,  7.69440178e-01],\n",
       "        [-9.41054169e-01,  1.15285354e+00,  7.02797756e-01,\n",
       "         -1.09487928e-01, -9.19960185e-02,  1.06114910e+00,\n",
       "          4.67734879e-01, -1.01309107e+00],\n",
       "        [-1.76780805e+00,  1.42864669e+00,  1.31629191e+00,\n",
       "         -9.92000835e-02, -9.11354031e-01,  6.77554551e-01,\n",
       "          8.30801187e-02, -1.32790556e+00],\n",
       "        [ 9.24569073e-02, -8.38312381e-01, -4.33212351e-01,\n",
       "          8.95238238e-02,  2.87113878e-01, -9.86410272e-01,\n",
       "          7.81094220e-03,  2.40954097e-01],\n",
       "        [-1.32878419e-02, -1.48043758e-02,  3.67541141e-02,\n",
       "          1.62967210e-01, -4.33197619e-02, -1.16632454e-01,\n",
       "          2.41734805e-02, -4.28451963e-02],\n",
       "        [ 4.57760592e-01, -1.98163509e-06, -1.11812435e-01,\n",
       "          4.33971459e-02, -7.43568073e-01, -8.79740258e-02,\n",
       "         -2.65362450e-01, -7.69047563e-01],\n",
       "        [-4.85603419e-02,  1.42349213e+00,  1.54984494e+00,\n",
       "          2.28854168e-01, -1.31581173e+00,  1.04451364e+00,\n",
       "         -1.40887721e+00,  7.95326809e-01],\n",
       "        [ 2.95433743e-01,  2.53047421e-01,  3.32062095e-01,\n",
       "         -1.03631960e-01,  6.53550571e-01,  1.31238065e-01,\n",
       "         -5.86887268e-01,  1.63785604e+00],\n",
       "        [ 1.05931842e+00, -7.08177013e-01, -6.74640795e-01,\n",
       "          4.19686803e-02, -3.81056913e-01, -3.39316377e-01,\n",
       "         -1.37546291e+00,  8.77793298e-01],\n",
       "        [-1.17336329e+00,  1.75311054e-01, -5.49894206e-01,\n",
       "          1.27976281e-01,  9.07919025e-01,  2.71507136e-01,\n",
       "          1.07868731e+00, -1.64383849e+00],\n",
       "        [ 1.78301328e+00, -1.21893763e+00, -1.82753325e+00,\n",
       "         -9.56633171e-02,  8.71680595e-01, -7.54691381e-01,\n",
       "         -1.25033629e+00,  1.89402738e+00],\n",
       "        [ 7.34916572e-02,  1.27786093e+00,  6.36806250e-01,\n",
       "         -7.55435165e-03, -1.05429415e+00,  1.44517725e+00,\n",
       "         -1.13880515e+00,  6.74681696e-01],\n",
       "        [-9.56913040e-03,  9.71194957e-03,  1.33686479e-02,\n",
       "         -8.79500545e-02, -5.98533337e-03,  2.43602077e-02,\n",
       "         -3.28196133e-04, -6.33769781e-03]]),\n",
       " array([[-1.6833552 , -1.28882469,  0.36546655, -1.17046824,  0.23415723,\n",
       "         -1.44532964, -0.98278876,  1.31969046],\n",
       "        [ 0.88389629,  1.02440727, -1.06172991,  0.74821094,  1.22340352,\n",
       "          0.62007764,  0.90466631, -0.79112495],\n",
       "        [ 0.55099651,  1.17331196, -1.14097226,  0.84575848,  0.80854366,\n",
       "          0.66114212,  1.06889091, -0.75309246],\n",
       "        [ 0.12909504, -0.091605  , -0.00926412, -0.15081723,  0.13250889,\n",
       "          0.13906749,  0.24437117, -0.25109546],\n",
       "        [ 0.51709523, -0.80865835,  1.50449673, -0.27892965, -1.49076485,\n",
       "          0.46065656, -1.48717861,  0.43405388],\n",
       "        [ 1.04119328,  1.12695965, -0.76042305,  0.81684197,  0.93065477,\n",
       "          0.79340066,  1.10325714, -0.98954291],\n",
       "        [ 1.47192538, -0.16015414,  0.96816839,  0.31853781, -1.28392563,\n",
       "          1.14092014, -1.1717852 , -1.10281879],\n",
       "        [-1.81860552, -1.44713579, -1.13067335, -1.54636153,  1.64100812,\n",
       "         -1.77232446, -0.46489073,  1.60983081]]),\n",
       " array([[ 0.24218831, -1.84018279,  0.8484139 ],\n",
       "        [ 0.91515474, -1.29172633, -0.11644037],\n",
       "        [-1.78179151,  0.72912559,  1.5826072 ],\n",
       "        [ 0.85532169, -1.88380779,  0.09360855],\n",
       "        [ 1.40484829,  0.86397065, -1.71488678],\n",
       "        [ 0.23686451, -2.20933011,  0.65214098],\n",
       "        [ 1.35905765, -0.796998  , -0.91351931],\n",
       "        [-1.21767586,  1.84491699, -0.42545288]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "YAdAAg7AN3Hm",
    "outputId": "691571c1-3778-4d56-a76a-e0907c8170da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.69797648, -1.43037038, -1.98512144,  0.07614826,  0.89221706,\n",
       "        -0.66330959, -0.31453464,  2.06311709]),\n",
       " array([ 0.24291929,  0.00357639,  0.57397273,  0.25403998,  0.16104029,\n",
       "         0.29628113, -0.20232746, -0.46541479]),\n",
       " array([-0.78129943,  1.93498603,  0.06325396])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "P5_1ACCZOIhJ",
    "outputId": "2157bbe7-77eb-4889-8e30-0eb6ede340af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pHDUyZqwOM7a",
    "outputId": "8063e1d9-d487-43ff-ac4f-3d204ec03373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_outputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "10D1sM9IOTGG",
    "outputId": "45975015-2fa2-4bc0-fd60-4fecc35a2a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTaTfbXNmUrn"
   },
   "source": [
    "## Neural network (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zVsg-V0pOy5_",
    "outputId": "1f09745f-91d2-4057-b20e-975eb5edc27e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "xJmwyvk7O5kp",
    "outputId": "e85348a0-80c2-4850-b9cf-90fe24f33f8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1, 2, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = network.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "Zjt0xKZDPNlw",
    "outputId": "fa4f6e10-36ce-4f49-8315-9e6ca2b55ebf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lkCZGjnVPdU3",
    "outputId": "577d2017-3fc2-4f6d-e61c-6e9f65102e78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "zSqMoc33Pvkz",
    "outputId": "acc83284-a39e-4cf2-9133-0cc315f4a21a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0],\n",
       "       [ 0, 15,  1],\n",
       "       [ 0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkf_QRkpmZ7u"
   },
   "source": [
    "## Neural network (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "pwunTr7AQ7vI",
    "outputId": "d5e00f21-6641-4926-ea37-a67fbbe72e8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.352e+01, 3.170e+00, 2.720e+00, 2.350e+01, 9.700e+01, 1.550e+00,\n",
       "        5.200e-01, 5.000e-01, 5.500e-01, 4.350e+00, 8.900e-01, 2.060e+00,\n",
       "        5.200e+02]),\n",
       " 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ZGUxWMJMREcK",
    "outputId": "c7477033-9cf2-4d85-9f05-e2baf3d53b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bzFuBVn_RLGT",
    "outputId": "b0d29807-289c-4cef-8aac-1438ecb78ef8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = X_test[0].reshape(1, -1)\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "DzfNwQSMRUNX",
    "outputId": "0233366f-7458-49e7-e65c-d264d9210cd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.352e+01, 3.170e+00, 2.720e+00, 2.350e+01, 9.700e+01, 1.550e+00,\n",
       "        5.200e-01, 5.000e-01, 5.500e-01, 4.350e+00, 8.900e-01, 2.060e+00,\n",
       "        5.200e+02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oSxnsC4yRZn0",
    "outputId": "a8483aaf-d9cc-45a7-ff62-80865f850503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "A2xAgLwmRfDB",
    "outputId": "6f38393a-323d-4d79-b98a-6ddf020dbdea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target_names[network.predict(new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
